{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Vector Space Model (VSM) and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next weeks, we are going to re-implement Sherin's algorithm and apply it to the text data we've been working on last week! Here's our roadmap:\n",
    "\n",
    "**Week 6 - vectorization and linear algebra**\n",
    "6. Dampen: weight the frequency of words (1 + log[count])\n",
    "7. Scale: Normalize weighted frequency of words\n",
    "8. Direction: compute deviation vectors\n",
    "\n",
    "**Week 7 - Clustering**\n",
    "9. apply different unsupervised machine learning algorithms\n",
    "    * figure out how many clusters we want to keep\n",
    "    * inspect the results of the clustering algorithm\n",
    "\n",
    "**Week 8 - Visualizing the results**\n",
    "10. create visualizations to compare documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 5 - DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../week4-data-cleaning-Erin-c/papers/paper12.txt', '../week4-data-cleaning-Erin-c/papers/paper5.txt', '../week4-data-cleaning-Erin-c/papers/paper4.txt', '../week4-data-cleaning-Erin-c/papers/paper13.txt', '../week4-data-cleaning-Erin-c/papers/paper11.txt', '../week4-data-cleaning-Erin-c/papers/paper6.txt', '../week4-data-cleaning-Erin-c/papers/paper7.txt', '../week4-data-cleaning-Erin-c/papers/paper10.txt', '../week4-data-cleaning-Erin-c/papers/paper14.txt', '../week4-data-cleaning-Erin-c/papers/paper3.txt', '../week4-data-cleaning-Erin-c/papers/paper2.txt', '../week4-data-cleaning-Erin-c/papers/paper15.txt', '../week4-data-cleaning-Erin-c/papers/paper0.txt', '../week4-data-cleaning-Erin-c/papers/paper1.txt', '../week4-data-cleaning-Erin-c/papers/paper16.txt', '../week4-data-cleaning-Erin-c/papers/paper9.txt', '../week4-data-cleaning-Erin-c/papers/paper8.txt']\n"
     ]
    }
   ],
   "source": [
    "# using glob, find all the text files in the \"Papers\" folder\n",
    "import glob\n",
    "\n",
    "files = glob.glob('../week4-data-cleaning-Erin-c/papers/*.txt')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# get all the data from the text files into the \"documents\" list\n",
    "# P.S. make sure you use the 'utf-8' encoding\n",
    "documents = []\n",
    "\n",
    "for filename in files: \n",
    "    with open (filename, \"r\", encoding='utf-8') as f:\n",
    "        documents.append(f.read())\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n103\\n\\n\\x0cepistemic network analysis and topic modeling for chat\\ndata from collaborative learning environment\\nzhiqiang cai\\n\\nbrendan eagan\\n\\nnia m. dowell\\n\\nthe university of memphis\\n365 innovation drive, suite 410\\nmemphis, tn, usa\\n\\nuniversity of wisconsin-madison\\n1025 west johnson street\\nmadison, wi, usa\\n\\nthe university of memphis\\n365 innovation drive, suite 410\\nmemphis, tn, usa\\n\\nzcai@memphis.edu\\n\\neaganb@gmail.com\\n\\nniadowell@gmail.com\\n\\njames w. pennebaker\\n\\ndavid w. shaffer\\n\\narthur c. graesser\\n\\nuniversity of texas-austin\\n116 inner campus dr stop g6000\\naustin, tx, usa\\n\\nuniversity of wisconsin-madison\\n1025 west johnson street\\nmadison, wi, usa\\n\\nthe university of memphis\\n365 innovation drive, suite 403\\nmemphis, tn, usa\\n\\npennebaker@utexas.edu\\n\\ndws@education.wisc.edu\\n\\nart.graesser@gmail.com\\n\\nabstract\\nthis study investigates a possible way to analyze chat data from\\ncollaborative learning environments using epistemic network\\nanalysis and topic modeling. a 300-topic general topic model\\nbuilt from tasa (touchstone applied science associates) corpus was used in this study. 300 topic scores for each of the 15,670\\nutterances in our chat data were computed. seven relevant topics\\nwere selected based on the total document scores. while the aggregated topic scores had some power in predicting students‚Äô\\nlearning, using epistemic network analysis enables assessing the\\ndata from a different angle. the results showed that the topic\\nscore based epistemic networks between low gain students and\\nhigh gain'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 1000 characters of the first document to see what it \n",
    "# looks like (we'll use this as a sanity check below)\n",
    "documents[0][:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40419 34802\n",
      "37246 32790\n",
      "44069 40060\n",
      "45290 42279\n",
      "32309 28230\n",
      "47883 41326\n",
      "42649 35126\n",
      "49209 42649\n",
      "40687 32758\n",
      "47409 43006\n",
      "46793 42281\n",
      "31602 28154\n",
      "50073 39342\n",
      "41142 35542\n",
      "42078 37673\n",
      "47877 44087\n",
      "45756 39971\n"
     ]
    }
   ],
   "source": [
    "# only select the text that's between the first occurence of the \n",
    "# the word \"abstract\" and the last occurence of the word \"reference\"\n",
    "# Optional: print the length of the string before and after, as a \n",
    "# sanity check\n",
    "# HINT: https://stackoverflow.com/questions/14496006/finding-last-occurrence-of-substring-in-string-replacing-that\n",
    "# read more about rfind: https://www.tutorialspoint.com/python/string_rfind.htm\n",
    "\n",
    "for i,doc in enumerate(documents):\n",
    "    print(len(documents[i]), end=' ')\n",
    "    # only keep the text after the abstract\n",
    "    doc = doc[doc.index('abstract'):doc.rfind('reference')]\n",
    "    # save the result\n",
    "    documents[i] = doc\n",
    "    # print the length of the resulting string\n",
    "    print(len(documents[i]))\n",
    "    \n",
    "# one liner:\n",
    "# documents = [doc[doc.index('abstract'):doc.rfind('reference')] for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling. a 300-topic general topic model built from tasa (touchstone applied science associates) corpus was used in this study. 300 topic scores for each of the 15,670 utterances in our chat data were computed. seven relevant topics were selected based on the total document scores. while the aggregated topic scores had some power in predicting students‚Äô learning, using epistemic network analysis enables assessing the data from a different angle. the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different (ùë° = 2.00). overall, the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions.  keywords chat; collaborative learning; topic modelin\n"
     ]
    }
   ],
   "source": [
    "# replace carriage returns (i.e., \"\\n\") with a white space\n",
    "# check that the result looks okay by printing the \n",
    "# first 1000 characters of the 1st doc:\n",
    "\n",
    "documents = [doc.replace('\\n', ' ') for doc in documents]\n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling  a 300 topic general topic model built from tasa  touchstone applied science associates  corpus was used in this study  300 topic scores for each of the 15 670 utterances in our chat data were computed  seven relevant topics were selected based on the total document scores  while the aggregated topic scores had some power in predicting students  learning  using epistemic network analysis enables assessing the data from a different angle  the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different  ùë°   2 00   overall  the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions   keywords chat  collaborative learning  topic modelin\n"
     ]
    }
   ],
   "source": [
    "# replace the punctation below by a white space\n",
    "# check that the result looks okay \n",
    "# (e.g., by print the first 1000 characters of the 1st doc)\n",
    "\n",
    "punctuation_marks = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '‚àí', '‚Äù', '‚Äú', '‚Äô']\n",
    "\n",
    "\n",
    "# remove ponctuation\n",
    "for i,doc in enumerate(documents): \n",
    "    for punc in punctuation_marks: \n",
    "        doc = doc.replace(punc, ' ')\n",
    "    documents[i] = doc\n",
    "    \n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract there is a critical need to develop new educational technology applications that analyze the data collected by universities to ensure that students graduate in a timely fashion   to  years   and they are well prepared for jobs in their respective fields of study  in this paper  we present a novel approach for analyzing historical educational records from a large  public university to perform next term grade prediction  i e   to estimate the grades that a student will get in a course that he she will enroll in the next term  accurate next term grade prediction holds the promise for better student degree planning  personalized advising and automated interventions to ensure that students stay on track in their chosen degree program and graduate on time  we present a factorization based approach called matrix factorization with temporal course wise influence that incorporates course wise influence effects and temporal effects for grade prediction  in this model  students and cours\n"
     ]
    }
   ],
   "source": [
    "# remove numbers by either a white space or the word \"number\"\n",
    "# again, print the first 1000 characters of the first document\n",
    "# to check that you're doing the right thing\n",
    "for i,doc in enumerate(documents): \n",
    "    for num in range(10):\n",
    "        doc = doc.replace(str(num), '')\n",
    "    documents[i] = doc\n",
    "\n",
    "print(documents[1][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling   topic general topic model built tasa  touchstone applied science associates  corpus used study   topic scores   utterances chat data computed  seven relevant topics selected based total document scores  aggregated topic scores power predicting students  learning  using epistemic network analysis enables assessing data different angle  results showed topic score based epistemic networks low gain students high gain students significantly different  ùë°       overall  results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epistemic network analysis    introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchronousl\n"
     ]
    }
   ],
   "source": [
    "# Remove the stop words below from our documents\n",
    "# print the first 1000 characters of the first document\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "\n",
    "\n",
    "# remove stop words\n",
    "for i,doc in enumerate(documents):\n",
    "    for stop_word in stop_words:\n",
    "        doc = doc.replace(' ' + stop_word + ' ', ' ')\n",
    "    documents[i] = doc\n",
    "\n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling topic general topic model built tasa touchstone applied science associates corpus used study topic scores utterances chat data computed seven relevant topics selected based total document scores aggregated topic scores power predicting students learning using epistemic network analysis enables assessing data different angle results showed topic score based epistemic networks low gain students high gain students significantly different overall results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions keywords chat collaborative learning topic modeling epistemic network analysis introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchronously asynchronously participate ta\n"
     ]
    }
   ],
   "source": [
    "# remove words with one and two characters (e.g., 'd', 'er', etc.)\n",
    "# print the first 1000 characters of the first document\n",
    "\n",
    "for i,doc in enumerate(documents):  \n",
    "    doc = [x for x in doc.split() if len(x) > 2]\n",
    "    doc = \" \".join(doc)\n",
    "    documents[i] = doc\n",
    "\n",
    "print(documents[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package all of your work above into a function that cleans a given document\n",
    "\n",
    "# def clean_list_of_documents(documents):\n",
    "    \n",
    "#     cleaned_docs = []\n",
    "\n",
    "#     for i,doc in enumerate(documents):\n",
    "#         # only keep the text after the abstract\n",
    "#         doc = doc[doc.index('abstract'):]\n",
    "#         # only keep the text before the references\n",
    "#         doc = doc[:doc.rfind('reference')]\n",
    "#         # replace return carriage with white space\n",
    "#         doc = doc.replace('\\n', ' ')\n",
    "#         # remove ponctuation\n",
    "#         for punc in punctuation: \n",
    "#             doc = doc.replace(punc, ' ')\n",
    "#         # remove numbers\n",
    "#         for i in range(10):\n",
    "#             doc = doc.replace(str(i), ' ')\n",
    "#         # remove stop words\n",
    "#         for stop_word in stop_words:\n",
    "#             doc = doc.replace(' ' + stop_word + ' ', ' ')\n",
    "#         # remove single characters and stem the words \n",
    "#         doc = [x for x in doc.split() if len(x) > 2]\n",
    "#         doc = \" \".join(doc)\n",
    "#         # save the result to our list of documents\n",
    "#         cleaned_docs.append(doc)\n",
    "        \n",
    "#     return cleaned_docs\n",
    "\n",
    "# functions from week #5\n",
    "import re\n",
    "\n",
    "def abbreviate(documents):\n",
    "    abbreviated_texts = []\n",
    "    for document in documents:\n",
    "        term1 = document.find('abstract')\n",
    "        term2 = document.rfind('reference')\n",
    "        substring = document[term1:term2]\n",
    "        abbreviated_texts.append(substring)\n",
    "    return abbreviated_texts\n",
    "\n",
    "def strip_new_lines(documents):\n",
    "    remove_returns = []\n",
    "    for document in documents:\n",
    "        substring = document.replace('\\n', ' ')\n",
    "        remove_returns.append(substring)\n",
    "    return remove_returns \n",
    "\n",
    "def strip_punctuation(documents):\n",
    "    clean_punctuations = []\n",
    "    for document in documents:\n",
    "        for punctuation_mark in punctuation_marks:\n",
    "            document = document.replace(punctuation_mark, \" \")\n",
    "        clean_punctuations.append(document)\n",
    "    return clean_punctuations\n",
    "\n",
    "def strip_numbers(documents):\n",
    "    clean_numbers = []\n",
    "    for document in documents:\n",
    "        for char in document:\n",
    "            if char.isdigit():\n",
    "                document = document.replace(char, \" \")\n",
    "        clean_numbers.append(document)\n",
    "    return clean_numbers\n",
    "\n",
    "def strip_stopwords(documents):\n",
    "    clean_stopwords = []\n",
    "    for document in documents:\n",
    "        words = document.split()\n",
    "        resultwords  = [word for word in words if word.lower() not in stop_words]\n",
    "        result = ' '.join(resultwords)\n",
    "        clean_stopwords.append(result)\n",
    "    return clean_stopwords\n",
    "\n",
    "def strip_short_words(documents):\n",
    "    clean_shortwords = []\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    for document in documents:\n",
    "        document = shortword.sub('', document)\n",
    "        clean_shortwords.append(document)\n",
    "    return clean_shortwords\n",
    "\n",
    "def clean_list_of_documents(documents):\n",
    "    cleaned_docs = []\n",
    "    \n",
    "    # only looks at text between abstract and reference\n",
    "    documents = abbreviate(documents)\n",
    "    \n",
    "    # replaces new lines with a space\n",
    "    documents = strip_new_lines(documents)\n",
    "    \n",
    "    # replaces punctuation with a space\n",
    "    documents = strip_punctuation(documents)\n",
    "\n",
    "    # replaces numbers with a space\n",
    "    documents = strip_numbers(documents)\n",
    "   \n",
    "    # removes stop words\n",
    "    documents = strip_stopwords(documents)\n",
    "  \n",
    "    # removes words less than length 3\n",
    "    documents = strip_short_words(documents)\n",
    "    \n",
    "    cleaned_docs = documents\n",
    "\n",
    "    return cleaned_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling topic general topic model built tasa touchstone applied science associates corpus used study topic scores utterances chat data computed seven relevant topics selected based total document scores aggregated topic scores power predicting students learning using epistemic network analysis enables assessing data different angle results showed topic score based epistemic networks low gain students high gain students significantly different overall results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions keywords chat collaborative learning topic modeling epistemic network analysis introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchronously asynchronously participate ta\n"
     ]
    }
   ],
   "source": [
    "# reimport your raw data\n",
    "documents = []\n",
    "\n",
    "for filename in files: \n",
    "    with open (filename, \"r\", encoding='utf-8') as f:\n",
    "        documents.append(f.read())\n",
    "        \n",
    "# clean your files using the function above\n",
    "docs = clean_list_of_documents(documents)\n",
    "\n",
    "# print the first 1000 characters of the first document\n",
    "print(docs[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Build your list of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of words (i.e., the vocabulary) is going to become the columns of your matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5648\n",
      "['abilities', 'ability', 'able', 'abnormal', 'absence', 'absent', 'absolute', 'abstract', 'abstracts', 'academic', 'acc', 'accelerated', 'accentuate', 'accept', 'access', 'accessed', 'accesses', 'accessible', 'accommodate', 'accomplish', 'accordance', 'according', 'accordingly', 'account', 'accounting', 'accounts', 'accuracies', 'accuracy', 'accurate', 'accurately', 'achievable', 'achieve', 'achieved', 'achievement', 'achiever', 'achieves', 'achieving', 'acids', 'acknowledgements', 'acknowledging', 'acknowledgments', 'acm', 'acoustic', 'acquire', 'acquired', 'acquisition', 'across', 'act', 'acting', 'action', 'actionable', 'actioncount', 'actions', 'activate', 'activated', 'activation', 'activations', 'active', 'actively', 'activities', 'activity', 'actors', 'actual', 'actually', 'adam', 'adapt', 'adaptability', 'adaptation', 'adapted', 'adapting', 'adaptive', 'adaptively', 'adapts', 'adasyn', 'add', 'added', 'adding', 'addition', 'additional', 'additionally', 'additive', 'address', 'addressed', 'addresses', 'addressing', 'adds', 'adeetee', 'adhering', 'adjacency', 'adjacent', 'adjectives', 'adjust', 'adjusted', 'adjusting', 'adjustment', 'adjustments', 'administered', 'admm', 'adobe', 'adolescence', 'adopt', 'adopted', 'adopting', 'adoption', 'adoptions', 'adopts', 'adult', 'adulthood', 'adults', 'advance', 'advanced', 'advancement', 'advancements', 'advances', 'advancing', 'advantage', 'advantages', 'adversity', 'advice', 'advising', 'advisors', 'affect', 'affected', 'affecting', 'affective', 'affects', 'affiliated', 'affine', 'afford', 'affordance', 'afforded', 'affords', 'afm', 'aforementioned', 'african', 'age', 'aged', 'agent', 'agentbased', 'agents', 'ages', 'aggarwal', 'agglomeration', 'agglomerative', 'aggregate', 'aggregated', 'aggregating', 'aggregation', 'aggressively', 'agnostic', 'ago', 'agreed', 'agreement', 'agrifolia', 'ahead', 'aic', 'aicoptimal', 'aid', 'aim', 'aims', 'air', 'ait', 'akaike', 'albeit', 'alert', 'aleven', 'algebra', 'algorithm', 'algorithms', 'align', 'aligned', 'aligning', 'alignment', 'alike', 'allessio', 'alleviates', 'allocation', 'allow', 'allowed', 'allowing', 'allows', 'ally', 'almost', 'alone', 'along', 'alongside', 'aloud', 'alphabetical', 'already', 'alright', 'also', 'altered', 'alternate', 'alternating', 'alternative', 'alternatively', 'alternatives', 'although', 'altogether', 'always', 'amazon', 'ambiguity', 'amenable', 'american', 'amidst', 'amino', 'among', 'amongst', 'amount', 'amounts', 'amplitude', 'amply', 'analogical', 'analogous', 'analogue', 'analyses', 'analysis', 'analytic', 'analytical', 'analytics', 'analyze', 'analyzed', 'analyzing', 'ancestor', 'anchor', 'ancova', 'andersen', 'anderson', 'andre', 'anecdotally', 'anger', 'angle', 'angles', 'animal', 'animals', 'animalwatch', 'animated', 'ann', 'annotate', 'annotated', 'annotating', 'annotation', 'annotations', 'annu', 'anonymized', 'another', 'answer', 'answered', 'answering', 'answers', 'anti', 'anticipated', 'anxiety', 'anxious', 'anything', 'anywhere', 'apart', 'api', 'app', 'apparent', 'appcount', 'appealing', 'appear', 'appeared', 'appearing', 'appears', 'appendix', 'applicability', 'applicable', 'application', 'applications', 'applied', 'applies', 'apply', 'applying', 'approach', 'approaches', 'approaching', 'appropriate', 'appropriateness', 'approximated', 'approximately', 'aptitude', 'aptitudetreatment', 'arbitrary', 'arbor', 'archetypes', 'architecture', 'architectures', 'area', 'areas', 'aren', 'arenas', 'arguable', 'argue', 'argued', 'argument', 'ari', 'arise', 'arises', 'arithmetic', 'arithmetical', 'arithmetics', 'armed', 'army', 'around', 'arousal', 'arranged', 'arrangement', 'arrive', 'arrives', 'arrogance', 'arrow', 'arrows', 'art', 'artefacts', 'article', 'articles', 'articulated', 'artificial', 'artificially', 'arts', 'ascending', 'ascertain', 'asian', 'asked', 'asking', 'asks', 'asmaa', 'aspect', 'aspects', 'aspx', 'assess', 'assessed', 'assesses', 'assessing', 'assessment', 'assessments', 'assiduous', 'assign', 'assigned', 'assigning', 'assignment', 'assignments', 'assigns', 'assimilate', 'assist', 'assistance', 'assistants', 'assisting', 'associated', 'associates', 'associating', 'association', 'associations', 'assume', 'assumed', 'assumes', 'assumption', 'assumptions', 'assured', 'asymmetric', 'asynchronously', 'ati', 'atlanta', 'attains', 'attempt', 'attempted', 'attempting', 'attempts', 'attend', 'attendant', 'attending', 'attention', 'attentional', 'attentionaware', 'attentivereview', 'attitude', 'attitudes', 'attracted', 'attractive', 'attributable', 'attribute', 'attributed', 'attributes', 'attrition', 'auc', 'aucs', 'audible', 'audio', 'auditory', 'augment', 'augmented', 'aupr', 'auroc', 'aus', 'authentic', 'author', 'authoring', 'authors', 'auto', 'autoassociator', 'autoencoder', 'autoencoders', 'automate', 'automated', 'automatic', 'automatically', 'automaticity', 'automation', 'automatized', 'autonomy', 'autotutor', 'auxiliary', 'availability', 'available', 'avenue', 'avenues', 'average', 'averaged', 'averaging', 'avg', 'avgtime', 'avgtimeonstepps', 'avgtimeonstepsessionps', 'avoid', 'avoidance', 'avoided', 'avoiding', 'avoids', 'awarded', 'aware', 'away', 'axes', 'axis', 'azevedo', 'back', 'background', 'backgrounds', 'backpropagation', 'backspace', 'backward', 'backwards', 'bad', 'bad‚Äìgood', 'baker', 'bakhtiari', 'balance', 'balanced', 'balloon', 'bandit', 'bandpass', 'bands', 'bandwidth', 'bank', 'banks', 'bar', 'barnes', 'bars', 'bartle', 'base', 'based', 'baseline', 'baselines', 'basic', 'basically', 'basics', 'basis', 'basu', 'batch', 'bayes', 'bayesian', 'bear', 'beating', 'beauty', 'became', 'beck', 'become', 'becomes', 'becoming', 'beep', 'began', 'begin', 'beginning', 'begins', 'behave', 'behaves', 'behavior', 'behavioral', 'behaviors', 'behaviours', 'behind', 'belief', 'beliefs', 'believe', 'believed', 'belong', 'belongs', 'beneficial', 'benefit', 'benefited', 'benefits', 'benjamini', 'berlin', 'bernoulli', 'besides', 'best', 'beta', 'better', 'beyond', 'bhide', 'bias', 'biased', 'bic', 'big', 'bigger', 'biggest', 'bigsmall', 'big‚Äìsmall', 'billion', 'billy', 'bin', 'binary', 'biochemistry', 'biochemists', 'biol', 'biology', 'biostatistics', 'biserial', 'bixler', 'bkt', 'black', 'blackboard', 'blank', 'blind', 'blink', 'blinks', 'blogs', 'blood', 'bloom', 'blue', 'bodies', 'body', 'bold', 'bolded', 'bonds', 'bonferroni', 'book', 'boost', 'boosted', 'boosting', 'boosts', 'boots', 'bored', 'boredom', 'borja', 'bottom', 'bouchet', 'bound', 'boundaries', 'boundary', 'bounded', 'bounds', 'boutilier', 'box', 'boxes', 'boy', 'boyce', 'boyd', 'boys', 'brain', 'branch', 'branched', 'branches', 'branching', 'brands', 'break', 'breaking', 'breaks', 'brevity', 'brick', 'bridge', 'bridges', 'brief', 'briefly', 'bring', 'brings', 'brno', 'broad', 'broader', 'broadly', 'brow', 'browse', 'browser', 'browsing', 'brute', 'bubbles', 'budget', 'build', 'building', 'builds', 'built', 'bundles', 'burdensome', 'burger', 'buried', 'burleson', 'business', 'button', 'buttons', 'bydzÃåovskaÃÅ', 'bytes', 'b¬µz', 'bœÉz', 'cade', 'cai', 'calcularis', 'calculate', 'calculated', 'calculates', 'calculating', 'calculation', 'calculations', 'calculus', 'calibrate', 'calibrating', 'calibration', 'california', 'call', 'called', 'came', 'camera', 'cameras', 'canada', 'candidate', 'cannot', 'canonical', 'capabilities', 'capability', 'capable', 'capital', 'capped', 'caps', 'capture', 'captured', 'captures', 'capturing', 'car', 'care', 'career', 'careful', 'carefully', 'careless', 'carl', 'carnegie', 'carolina', 'carried', 'carry', 'cascade', 'case', 'cases', 'catalog', 'catch', 'categorical', 'categories', 'categorization', 'categorizations', 'categorize', 'categorized', 'category', 'caucasian', 'caught', 'causal', 'causality', 'cause', 'caused', 'causes', 'causing', 'caution', 'cautious', 'cdf', 'cdfs', 'ceie', 'ceiling', 'cell', 'cellij', 'cells', 'cell‚àó', 'center', 'centers', 'central', 'centrally', 'centroid', 'centroids', 'cert', 'certain', 'certificate', 'certification', 'cfa', 'chain', 'chains', 'challenge', 'challenges', 'challenging', 'chance', 'chances', 'change', 'changed', 'changes', 'changing', 'channel', 'channels', 'chapter', 'chapters', 'character', 'characteristic', 'characteristics', 'characterization', 'characterizations', 'characterize', 'characterized', 'characterizes', 'characterizing', 'characters', 'charles', 'charlie', 'chart', 'charts', 'charu', 'chat', 'chatroom', 'chats', 'check', 'checkboxes', 'checked', 'checks', 'cheek', 'chein', 'chemical', 'chemistry', 'chenxi', 'chi', 'child', 'childcare', 'children', 'ching', 'choi', 'choice', 'choices', 'chong', 'choose', 'chooses', 'choosing', 'chose', 'chosen', 'chromebook', 'chromosomes', 'chronological', 'chu', 'chua', 'chull', 'cikm', 'circle', 'circles', 'circuit', 'circulatory', 'circumstanced', 'circumstances', 'city', 'civil', 'claim', 'claimed', 'claims', 'clarify', 'clark', 'class', 'classes', 'classi', 'classic', 'classical', 'classification', 'classifications', 'classified', 'classifier', 'classifiers', 'classify', 'classifying', 'classmates', 'classroom', 'classrooms', 'clean', 'clear', 'cleared', 'clearly', 'click', 'clicking', 'clicks', 'clickstream', 'clickstreams', 'client', 'climatic', 'clip', 'close', 'closed', 'closely', 'closeness', 'closer', 'closest', 'closing', 'cloze', 'cluster', 'clustered', 'clustering', 'clusterings', 'clusters', 'cnn', 'cntrl', 'code', 'coded', 'coding', 'coefficient', 'coefficients', 'cognition', 'cognitive', 'cognizant', 'cogtool', 'coh', 'cohen', 'cohesion', 'cohesive', 'col', 'collabo', 'collaborating', 'collaboration', 'collaborative', 'collapse', 'collapsed', 'collapsing', 'colleagues', 'collect', 'collected', 'collecting', 'collection', 'college', 'collin', 'collinearity', 'color', 'colored', 'column', 'columns', 'com', 'combination', 'combinations', 'combine', 'combined', 'combines', 'combining', 'come', 'comes', 'coming', 'commands', 'commensurate', 'comment', 'comments', 'commercial', 'commercialoff', 'committing', 'common', 'commonly', 'communication', 'community', 'compact', 'companion', 'companions', 'company', 'comparable', 'comparative', 'comparatively', 'compare', 'compared', 'compares', 'comparing', 'comparison', 'comparisons', 'compelling', 'compensated', 'compete', 'competence', 'competencies', 'competing', 'competition', 'competitiveness', 'compilation', 'compiled', 'complement', 'complementary', 'complements', 'complete', 'completed', 'completely', 'completeness', 'completing', 'completion', 'complex', 'complexity', 'compliance', 'complicated', 'complied', 'complimentary', 'comply', 'complying', 'component', 'components', 'compose', 'composed', 'composing', 'composite', 'composition', 'compounded', 'compounding', 'comprehended', 'comprehension', 'comprehensive', 'compression', 'comprised', 'compulsory', 'comput', 'computation', 'computational', 'computationally', 'computations', 'compute', 'computed', 'computer', 'computerbased', 'computerenabled', 'computerized', 'computers', 'computes', 'computing', 'con', 'conati', 'concatenated', 'concatenates', 'concentrate', 'concentrating', 'concentration', 'concept', 'concepts', 'conceptual', 'conceptualized', 'conceptually', 'concern', 'concerning', 'concerns', 'concise', 'concisely', 'conclude', 'concluded', 'concluding', 'conclusion', 'conclusions', 'concrete', 'concretely', 'concreteness', 'cond', 'condense', 'condensing', 'condition', 'conditional', 'conditions', 'conduct', 'conducted', 'conducting', 'conf', 'conference', 'confidence', 'confident', 'confidently', 'configuration', 'configurations', 'confirm', 'confirmed', 'confound', 'confounds', 'confronted', 'confused', 'confusion', 'congratulations', 'congruence', 'congruent', 'conjugate', 'connect', 'connected', 'connectedness', 'connection', 'connections', 'connector', 'connotation', 'conpliance', 'conscious', 'consecutive', 'consensus', 'consent', 'consequences', 'consequently', 'conservative', 'consider', 'considerable', 'considerably', 'consideration', 'considerations', 'considered', 'considering', 'considers', 'consist', 'consisted', 'consistency', 'consistent', 'consistently', 'consisting', 'consists', 'constant', 'constitutes', 'constrain', 'constrained', 'constraining', 'constraint', 'constraints', 'construct', 'constructed', 'constructing', 'construction', 'constructive', 'constructs', 'consultation', 'consumed', 'consumer', 'consumes', 'consumption', 'contain', 'contained', 'containing', 'contains', 'contemporary', 'contend', 'contender', 'content', 'contentbased', 'contents', 'context', 'contexts', 'contextual', 'continue', 'continued', 'continues', 'continuing', 'continuous', 'continuously', 'contrary', 'contrast', 'contribute', 'contributed', 'contributes', 'contributing', 'contribution', 'contributions', 'contrived', 'control', 'controlled', 'controlling', 'controls', 'controversy', 'conv', 'convenience', 'conventions', 'converge', 'convergence', 'convergent', 'converges', 'converging', 'conversational', 'conversations', 'conversely', 'conversion', 'convert', 'converted', 'converting', 'convex', 'convey', 'convl', 'convo', 'convolution', 'convolutional', 'convolutions', 'cool', 'coordinated', 'coordinates', 'coordinator', 'cope', 'copy', 'copyright', 'cordillera', 'core', 'corp', 'corpora', 'corporate', 'corpus', 'corr', 'correct', 'corrected', 'correcting', 'correction', 'corrective', 'correctly', 'correctlypmade', 'correctness', 'corrects', 'correlate', 'correlated', 'correlates', 'correlating', 'correlation', 'correlations', 'correspond', 'corresponded', 'correspondence', 'corresponding', 'corresponds', 'corroborated', 'cosine', 'cost', 'costly', 'costs', 'cots', 'could', 'council', 'count', 'counter', 'counterpart', 'counterparts', 'counting', 'countries', 'counts', 'coupled', 'coupling', 'courier', 'course', 'coursera', 'courses', 'coursewise', 'covariate', 'cover', 'covered', 'covering', 'covers', 'covert', 'cpe', 'create', 'created', 'creates', 'creating', 'creation', 'creative', 'credit', 'criteria', 'criterion', 'critical', 'cross', 'crosscontext', 'crosscorpus', 'crossing', 'crosstrained', 'crosstraining', 'crossvalidation', 'crowd', 'crowdsources', 'crowdsourcing', 'crucial', 'csc', 'cts', 'cue', 'cues', 'culture', 'cummings', 'cumulative', 'curious', 'current', 'currently', 'curriculum', 'curve', 'curves', 'custom', 'cut', 'cutoff', 'cyber', 'cyberlearning', 'cycle', 'cycles', 'cynical', 'czech', 'dad', 'danger', 'danielle', 'dariush', 'dark', 'dashed', 'data', 'database', 'dataset', 'datasets', 'date', 'davidson', 'day', 'daydreaming', 'days', 'dbakhtiari', 'deal', 'dealing', 'dealt', 'dearth', 'debriefing', 'debshila', 'decade', 'decades', 'decay', 'decaying', 'december', 'decide', 'decided', 'deciding', 'decision', 'decisionmaking', 'decisions', 'declines', 'decoder', 'decoders', 'decompose', 'decomposed', 'decomposes', 'decomposition', 'decontextualized', 'decouple', 'decrease', 'decreased', 'decreases', 'decreasing', 'deemed', 'deep', 'deeper', 'deeply', 'default', 'defaults', 'deficiencies', 'deficiency', 'deficient', 'deficits', 'define', 'defined', 'defining', 'definition', 'definitions', 'degrades', 'degree', 'degrees', 'del', 'delay', 'delayed', 'delete', 'deliberate', 'delicate', 'delight', 'deliver', 'delivered', 'delivering', 'delivery', 'delta', 'demand', 'demographic', 'demographics', 'demonstrate', 'demonstrated', 'demonstrating', 'demonstration', 'demonstrations', 'demotivating', 'denied', 'denley', 'denominator', 'denote', 'denoted', 'denotes', 'denoting', 'dense', 'density', 'dental', 'department', 'departments', 'depend', 'dependencies', 'dependency', 'dependent', 'depending', 'depends', 'depict', 'depicted', 'depictions', 'depicts', 'deploy', 'deployed', 'deploying', 'deployment', 'deprives', 'dept', 'depth', 'depths', 'derivation', 'derivative', 'derive', 'derived', 'derives', 'deriving', 'descent', 'describe', 'described', 'describes', 'describing', 'description', 'descriptions', 'descriptive', 'deserves', 'design', 'designated', 'designed', 'designer', 'designers', 'designing', 'designs', 'desirable', 'desire', 'desired', 'desktop', 'despite', 'destroyed', 'detail', 'detailed', 'detailing', 'details', 'detect', 'detected', 'detecting', 'detection', 'detector', 'detectors', 'detects', 'determine', 'determined', 'determines', 'determining', 'deterministic', 'deterministically', 'detrimental', 'develop', 'developed', 'developers', 'developing', 'development', 'developmental', 'deviation', 'deviations', 'device', 'devices', 'devise', 'dft', 'diag', 'diagnose', 'diagnosed', 'diagnosis', 'diagnostic', 'diagnostically', 'diagram', 'diagrams', 'dialog', 'dialogs', 'dialogue', 'diameter', 'dichotomous', 'dictate', 'dictated', 'dictates', 'didn', 'diep', 'differ', 'differed', 'difference', 'differences', 'different', 'differential', 'differentially', 'differentiate', 'differentiates', 'differentiating', 'differently', 'differs', 'difficult', 'difficulties', 'difficulty', 'dig', 'digging', 'digit', 'digital', 'digitized', 'dilation', 'diligence', 'diligent', 'diligently', 'dimension', 'dimensional', 'dimensionality', 'dimensions', 'diminish', 'dimpler', 'ding', 'dip', 'direct', 'directed', 'directing', 'direction', 'directions', 'directly', 'directs', 'dirichlet', 'disabilities', 'disability', 'disabled', 'disadvantage', 'disappear', 'discard', 'discarded', 'discarding', 'discernable', 'discon', 'discount', 'discounted', 'discounting', 'discouragingly', 'discourse', 'discover', 'discovered', 'discovering', 'discovers', 'discovery', 'discrete', 'discretization', 'discretized', 'discriminate', 'discriminative', 'discuss', 'discussed', 'discussion', 'discussions', 'disease', 'diseases', 'disengagement', 'disorder', 'disorders', 'dispersion', 'display', 'displayed', 'displays', 'disposal', 'disregard', 'disruption', 'disruptive', 'dissemination', 'dissimilarity', 'dist', 'distance', 'distances', 'distill', 'distinct', 'distinctly', 'distinguish', 'distinguished', 'distinguishes', 'distinguishing', 'distracted', 'distraction', 'distributed', 'distribution', 'distributions', 'district', 'diverge', 'divergence', 'divergent', 'diverse', 'diversity', 'divide', 'divided', 'dividing', 'division', 'divisionlevel', 'dlso', 'doctor', 'doctors', 'document', 'documentary', 'documents', 'dod', 'doesn', 'dog', 'domain', 'domains', 'domainspecific', 'dominated', 'done', 'dosg', 'dot', 'dotted', 'double', 'douglasii', 'dowell', 'downloaded', 'downsampled', 'downsampling', 'downside', 'drag', 'dragging', 'dramatically', 'draw', 'drawback', 'drawbacks', 'drawn', 'drift', 'drifting', 'drive', 'driven', 'driver', 'drives', 'driving', 'drk', 'drl', 'drop', 'dropoff', 'dropout', 'dropped', 'dropping', 'drops', 'drugs', 'dscovar', 'dtrl', 'dts', 'dual', 'due', 'dull', 'duplicated', 'duration', 'durso', 'dweck', 'dyad', 'dyads', 'dynamic', 'dynamical', 'dynamically', 'dynamics', 'dyscalculia', 'eagle', 'earlier', 'earliest', 'early', 'earning', 'earth', 'ease', 'easier', 'easily', 'easiness', 'easy', 'eckstein', 'ecological', 'economic', 'economics', 'economy', 'edge', 'edges', 'edited', 'edition', 'edm', 'edtech', 'edu', 'education', 'educational', 'effect', 'effected', 'effective', 'effectively', 'effectiveness', 'effects', 'efficacy', 'efficiency', 'efficient', 'efficiently', 'effort', 'effortful', 'efforts', 'egg', 'eggs', 'eight', 'eighth', 'either', 'elaborated', 'elapsed', 'elbadrawy', 'elbow', 'elbows', 'elect', 'elective', 'electively', 'electives', 'electrical', 'electronic', 'electronics', 'elegant', 'elegantly', 'element', 'elementary', 'elements', 'elementwise', 'elicited', 'elide', 'eliding', 'eligibility', 'eligible', 'eliminate', 'eliminated', 'eliminates', 'eliminating', 'ell', 'elm', 'else', 'elsewhere', 'elusive', 'email', 'embedded', 'embedding', 'embeddings', 'emerge', 'emerged', 'emergence', 'emotient', 'emotion', 'emotional', 'emotions', 'empathic', 'empathy', 'emphasis', 'emphasize', 'emphasized', 'emphasizes', 'emphasizing', 'empirical', 'empirically', 'employ', 'employed', 'employees', 'employing', 'employs', 'empty', 'ena', 'enable', 'enabled', 'enables', 'enabling', 'enamored', 'encapsulate', 'encoded', 'encoder', 'encoders', 'encodes', 'encoding', 'encodings', 'encompass', 'encountered', 'encourage', 'encouraged', 'encourages', 'encouraging', 'encyclopedia', 'end', 'endeavor', 'ended', 'ending', 'ends', 'energy', 'enforce', 'engage', 'engaged', 'engagement', 'engagement‚Äì', 'engages', 'engaging', 'engender', 'engendered', 'engendering', 'engine', 'engineering', 'engineers', 'engines', 'english', 'enhance', 'enhanced', 'enhancements', 'enhancing', 'enjoying', 'enjoyment', 'enormous', 'enough', 'enriched', 'enroll', 'enrolled', 'ensemble', 'ensure', 'ensured', 'ensures', 'ensuring', 'enter', 'entered', 'entire', 'entirely', 'entropy', 'entry', 'enumerated', 'envelope', 'environment', 'environmental', 'environments', 'episode', 'epistemic', 'epistemology', 'epochs', 'equal', 'equally', 'equap', 'equated', 'equation', 'equations', 'equipment', 'equipped', 'equitable', 'equivalent', 'equivocal', 'eqœÜ', 'era', 'ered', 'eric', 'err', 'error', 'errors', 'ers', 'escape', 'especially', 'essay', 'essential', 'essentially', 'establish', 'established', 'establishing', 'esteem', 'estimate', 'estimated', 'estimates', 'estimating', 'estimation', 'etc', 'eth', 'ethics', 'ethnicity', 'ethz', 'euclid', 'euclidean', 'eureka', 'evaluate', 'evaluated', 'evaluating', 'evaluation', 'evaluations', 'even', 'evenly', 'event', 'events', 'eventually', 'ever', 'every', 'everyone', 'evidence', 'evident', 'evolution', 'evolvement', 'evolves', 'evolving', 'exact', 'exactly', 'exaggerate', 'exam', 'examination', 'examine', 'examined', 'examining', 'example', 'examples', 'exams', 'exceeds', 'excel', 'excellent', 'except', 'exception', 'excerpt', 'excessive', 'excessively', 'exchange', 'excising', 'excitement', 'exciting', 'excluded', 'excludes', 'exclusively', 'executed', 'execution', 'exemplar', 'exercise', 'exercises', 'exhausted', 'exhaustive', 'exhibit', 'exhibited', 'exhibiting', 'exhibits', 'exist', 'existing', 'exists', 'exp', 'expand', 'expanded', 'expanding', 'expect', 'expectation', 'expected', 'expecting', 'expensive', 'experience', 'experienced', 'experiencing', 'experiment', 'experimental', 'experimentally', 'experimentation', 'experimented', 'experimenter', 'experiments', 'expert', 'expertise', 'experts', 'explain', 'explained', 'explaining', 'explains', 'explanation', 'explanations', 'explanatory', 'explicit', 'explicitly', 'exploit', 'exploration', 'exploratory', 'explore', 'explored', 'explores', 'exploring', 'exponential', 'exported', 'exposed', 'exposition', 'expository', 'exposure', 'express', 'expressed', 'expression', 'expressions', 'expressive', 'expÃÇ', 'extend', 'extended', 'extending', 'extends', 'extension', 'extensions', 'extensive', 'extensively', 'extent', 'external', 'externally', 'extinction', 'extra', 'extract', 'extracted', 'extracting', 'extraction', 'extractions', 'extracts', 'extreme', 'extremely', 'extremes', 'eye', 'eyegaze', 'eyes', 'eyetracking', 'eyetribe', 'eyex', 'face', 'facebook', 'faces', 'facial', 'facilitate', 'facilitated', 'facilitates', 'facility', 'facing', 'fact', 'factor', 'factorial', 'factoring', 'factorization', 'factorizations', 'factors', 'facts', 'factual', 'faculty', 'fades', 'fail', 'failed', 'failing', 'failure', 'fair', 'fairly', 'falakmasir', 'fall', 'falling', 'falls', 'false', 'familiar', 'familiarity', 'famous', 'fantasies', 'fantasy', 'far', 'fashion', 'fast', 'faster', 'fat', 'fatigue', 'favor', 'favorably', 'favorite', 'feasibility', 'feasible', 'feasibly', 'feature', 'features', 'fed', 'feed', 'feedback', 'feedbacks', 'feedforward', 'feeds', 'feel', 'feeling', 'feelings', 'fell', 'felt', 'female', 'females', 'ferguson', 'fertile', 'fewer', 'fewest', 'fidelity', 'field', 'fields', 'fiers', 'fifth', 'fig', 'figure', 'figures', 'file', 'files', 'fill', 'filled', 'fillers', 'film', 'films', 'filter', 'filtered', 'filtering', 'filters', 'final', 'finally', 'financial', 'find', 'finding', 'findings', 'finds', 'fine', 'finegrained', 'finest', 'fingertip', 'finish', 'finished', 'finishes', 'finishing', 'finite', 'fired', 'first', 'fit', 'fits', 'fitted', 'fitting', 'five', 'fivepoint', 'fix', 'fixate', 'fixated', 'fixating', 'fixation', 'fixations', 'fixed', 'fixing', 'flash', 'flat', 'flexibility', 'flexible', 'flexibly', 'flourish', 'flow', 'fluency', 'fly', 'fmb', 'focus', 'focused', 'focuses', 'focusing', 'fold', 'folder', 'folders', 'folding', 'foldit', 'folds', 'follow', 'followed', 'following', 'follows', 'follows‚àó', 'food', 'force', 'forced', 'forces', 'forcing', 'forecasts', 'forefront', 'forest', 'form', 'formal', 'formalize', 'formally', 'formance', 'format', 'formative', 'formats', 'formed', 'former', 'forms', 'formula', 'formulas', 'formulate', 'formulation', 'forth', 'fortunately', 'forty', 'forum', 'forums', 'forward', 'forwarding', 'forwards', 'foster', 'fostering', 'found', 'foundation', 'foundational', 'foundations', 'four', 'fourfold', 'fourth', 'fraction', 'frame', 'frames', 'framework', 'france', 'frank', 'freddy', 'free', 'freedom', 'french', 'frequencies', 'frequency', 'frequencyinverse', 'frequent', 'frequently', 'fresh', 'freshmen', 'freshness', 'friends', 'front', 'frontier', 'fronts', 'frust', 'frustrated', 'frustrating', 'frustration', 'fulfill', 'full', 'fullrl', 'fullscale', 'fully', 'function', 'functional', 'functionalities', 'functionality', 'functionals', 'functions', 'function‚àó', 'fundamental', 'fundamentally', 'fundamentals', 'funding', 'furthermore', 'future', 'futuristic', 'f√©d√©rale', 'gagement', 'gain', 'gained', 'gaining', 'gains', 'game', 'gameplay', 'games', 'gaming', 'gamma', 'gap', 'gaps', 'garden', 'gate', 'gateways', 'gathering', 'gauge', 'gaussian', 'gave', 'gaze', 'gazetutor', 'geigle', 'gema', 'gender', 'gene', 'general', 'generalizability', 'generalizable', 'generalization', 'generalize', 'generalized', 'generalizes', 'generalizing', 'generally', 'generate', 'generated', 'generates', 'generating', 'generation', 'generative', 'generic', 'genes', 'genetic', 'genuine', 'genuinely', 'geographic', 'geometric', 'geometry', 'george', 'georgia', 'germany', 'gesture', 'gestures', 'get', 'gets', 'getting', 'gick', 'girl', 'github', 'give', 'given', 'gives', 'givesÃÇ', 'giving', 'gleaned', 'gleaning', 'global', 'glorot', 'gluck', 'gmu', 'gnu', 'goal', 'goals', 'goes', 'going', 'gold', 'goms', 'good', 'google', 'got', 'gracefully', 'grade', 'graded', 'grader', 'graders', 'grades', 'gradient', 'gradients', 'grading', 'gradual', 'gradually', 'graduate', 'graduation', 'graesser', 'grained', 'gram', 'grammar', 'gramming', 'grand', 'grandiose', 'grant', 'granular', 'granularities', 'granularity', 'graph', 'graphbased', 'graphical', 'graphics', 'graphs', 'grateful', 'gray', 'great', 'greater', 'greatest', 'greatly', 'greedily', 'greedy', 'green', 'grey', 'grid', 'grit', 'gross', 'grossm', 'ground', 'grounded', 'group', 'grouped', 'grouping', 'groupings', 'groups', 'grow', 'growing', 'grows', 'growth', 'gsu', 'guarantee', 'guaranteed', 'guess', 'guessing', 'guidance', 'guide', 'guided', 'guidelines', 'guiding', 'gunter', 'gupta', 'guru', 'gurututor', 'habits', 'habitual', 'haha', 'hahaha', 'half', 'halves', 'hana', 'hand', 'handle', 'handles', 'happened', 'happens', 'haptek', 'hard', 'harder', 'hardest', 'hardly', 'hardware', 'harmful', 'harmonic', 'hattie', 'head', 'headphone', 'headword', 'health', 'healthcare', 'healthy', 'heard', 'heart', 'heat', 'heavily', 'heavy', 'height', 'heightened', 'held', 'helium', 'hellinger', 'help', 'helped', 'helpful', 'helpful‚Äìharmful', 'helping', 'helpless', 'helps', 'hence', 'hereafter', 'heterogeneity', 'heterogeneous', 'heuristic', 'heuristics', 'hey', 'hidden', 'hierarchical', 'hierarchy', 'high', 'highand', 'highdimensional', 'higher', 'highest', 'highlight', 'highlighted', 'highlighting', 'highlights', 'highly', 'highperforming', 'hinders', 'hint', 'hintcount', 'hints', 'hiring', 'hispanic', 'historical', 'histories', 'history', 'hit', 'hmm', 'hoc', 'hochberg', 'hold', 'holdout', 'holds', 'holm', 'home', 'homepage', 'homes', 'homework', 'honestly', 'honors', 'hope', 'hopelessness', 'horizons', 'horizontal', 'hospital', 'host', 'hot', 'hottest', 'hour', 'hours', 'households', 'however', 'hsp', 'http', 'https', 'hull', 'human', 'humanities', 'hundred', 'hundreds', 'hurried', 'hurts', 'hutt', 'hwang', 'hybrid', 'hyper', 'hyperlink', 'hyperlinks', 'hypermedia', 'hyperplane', 'hypotheses', 'hypothesis', 'hypothesize', 'hypothesized', 'hypothetical', 'iaeng', 'iafm', 'ibkt', 'icon', 'icons', 'idea', 'ideal', 'ideally', 'ideas', 'identical', 'identifiable', 'identification', 'identified', 'identifier', 'identifiers', 'identifies', 'identify', 'identifying', 'identity', 'idf', 'idle', 'ids', 'ieee', 'ies', 'iglesias', 'ignore', 'ignoring', 'iii', 'iis', 'ill', 'illness', 'illnesshealthcare', 'illstructured', 'illstructuredness', 'illuminate', 'illustrate', 'illustrated', 'illustrates', 'illustration', 'image', 'images', 'imagine', 'imbalance', 'imbalanced', 'imm', 'immediate', 'immediately', 'immersive', 'immutable', 'impact', 'impacted', 'impacting', 'impaired', 'impairs', 'imperceptible', 'imperfect', 'implement', 'implementation', 'implementations', 'implemented', 'implementing', 'implications', 'implicit', 'implicitly', 'implied', 'implies', 'imply', 'implying', 'importance', 'important', 'importantly', 'impose', 'impossible', 'impractical', 'improve', 'improved', 'improvement', 'improvements', 'improves', 'improving', 'impulse', 'imputation', 'inadequate', 'inattention', 'inattentive', 'inc', 'incidental', 'include', 'included', 'includes', 'including', 'inclusion', 'incoming', 'incomplete', 'incongruent', 'inconsistent', 'incorporate', 'incorporated', 'incorporates', 'incorporating', 'incorporation', 'incorrect', 'incorrectly', 'increase', 'increased', 'increases', 'increasing', 'increasingly', 'incremental', 'incrementally', 'increments', 'indeed', 'independence', 'independent', 'independently', 'index', 'indexed', 'india', 'indicate', 'indicated', 'indicates', 'indicating', 'indication', 'indicative', 'indicator', 'indicators', 'individual', 'individualization', 'individualize', 'individualized', 'individualizing', 'individually', 'individuals', 'induce', 'induced', 'inducing', 'induction', 'industrial', 'ineffective', 'inequality', 'inevitably', 'inf', 'infer', 'inference', 'inferred', 'inferring', 'infers', 'inflate', 'inflation', 'influence', 'influenced', 'influences', 'influencing', 'info', 'inform', 'informatics', 'information', 'informative', 'informed', 'infrastructure', 'infrequently', 'ing', 'inherently', 'inhibitor', 'init', 'initial', 'initialization', 'initialize', 'initialized', 'initially', 'initiate', 'initiated', 'initiating', 'injury', 'inner', 'innovation', 'inof', 'input', 'inputs', 'inquiry', 'inquisitive', 'inquisitively', 'insensitive', 'insert', 'inside', 'insight', 'insights', 'insignificant', 'inspiration', 'inspire', 'inspired', 'instance', 'instances', 'instant', 'instead', 'institute', 'institutes', 'institution', 'institutions', 'instruct', 'instructed', 'instructing', 'instruction', 'instructional', 'instructions', 'instructor', 'instructorcentric', 'instructors', 'insufficient', 'int', 'integral', 'integrate', 'integrated', 'integrates', 'integrating', 'integration', 'intelligence', 'intelligent', 'intended', 'intensive', 'intention', 'intentional', 'inter', 'interact', 'interacted', 'interacting', 'interaction', 'interactions', 'interactive', 'interactively', 'interactivity', 'intercept', 'intercepts', 'interdisciplinary', 'interest', 'interested', 'interesting', 'interestingly', 'interests', 'interface', 'interfaces', 'interference', 'interferences', 'intermediate', 'intermixing', 'internal', 'internalizing', 'international', 'internet', 'interplay', 'interpolate', 'interpolated', 'interpolation', 'interpret', 'interpretability', 'interpretable', 'interpretation', 'interpreted', 'interpreting', 'interrupt', 'interrupted', 'interrupting', 'interruptions', 'interrupts', 'interspersed', 'interval', 'intervals', 'intervene', 'intervened', 'intervenes', 'intervening', 'intervention', 'interventions', 'interview', 'interviewed', 'intractable', 'intrinsic', 'introduce', 'introduced', 'introduces', 'introduction', 'introductory', 'intrusive', 'intuitive', 'intutor', 'invalid', 'invasive', 'invasiveness', 'invented', 'inverse', 'inversely', 'inverted', 'invest', 'investigate', 'investigated', 'investigates', 'investigating', 'investigation', 'investigations', 'investing', 'investment', 'invideo', 'invited', 'inviting', 'invoke', 'invoked', 'involuntary', 'involve', 'involved', 'involves', 'involving', 'irrelevant', 'irrespective', 'islander', 'iso', 'isolate', 'isolation', 'isomorphic', 'issue', 'issues', 'item', 'items', 'iteractions', 'iteration', 'iterations', 'iterative', 'iteratively', 'ith', 'itss', 'iuse', 'iwa', 'ized', 'jaccard', 'jack', 'jacob', 'january', 'jaw', 'jenny', 'jerry', 'job', 'jobs', 'joe', 'joining', 'joint', 'jonassen', 'jonathan', 'journeys', 'jth', 'judgement', 'judgements', 'judges', 'junk', 'justified', 'justify', 'kak', 'kak‚àó', 'kalman', 'kappa', 'kardan', 'karypis', 'kcs', 'kdimensional', 'keen', 'keep', 'keeping', 'keeps', 'kept', 'keras', 'kernel', 'kernels', 'key', 'keyboard', 'keys', 'keystroke', 'keystrokes', 'keyword', 'keywords', 'kim', 'kind', 'kinds', 'knn', 'know', 'knowing', 'knowledge', 'known', 'kopp', 'kpca', 'krathwohl', 'kriglstein', 'kris', 'kullback', 'kus', 'kvc', 'k√§ser', 'lab', 'label', 'labeled', 'labeler', 'labeling', 'labels', 'laborative', 'laboratory', 'lack', 'lacking', 'lacks', 'laid', 'lall√©', 'language', 'lapses', 'large', 'largely', 'larger', 'largest', 'larity', 'lasso', 'last', 'lasted', 'lasting', 'lastly', 'late', 'latency', 'latent', 'later', 'latino', 'latter', 'lauch', 'launch', 'launched', 'launching', 'lausanne', 'layer', 'layers', 'layout', 'layouts', 'lbw', 'lda', 'lds', 'lead', 'leaderboard', 'leaderboards', 'leading', 'leads', 'leaf', 'learn', 'learned', 'learner', 'learnercentered', 'learners', 'learning', 'learningoriented', 'learnlab', 'learns', 'learnt', 'least', 'leave', 'leaves', 'leaving', 'lecture', 'lectures', 'led', 'left', 'legitimizing', 'legs', 'leibler', 'length', 'lengths', 'less', 'lesser', 'let', 'letter', 'letters', 'level', 'levels', 'leverage', 'leveraged', 'leveraging', 'lexical', 'libraries', 'lid', 'lie', 'lieu', 'lieve', 'life', 'light', 'like', 'liked', 'likelihood', 'likelihoods', 'likely', 'likert', 'likes', 'likewise', 'liking', 'lim', 'limit', 'limitation', 'limitations', 'limited', 'limits', 'line', 'linear', 'linearly', 'lines', 'linguistic', 'link', 'linkage', 'linked', 'links', 'linucb', 'lip', 'liquids', 'list', 'listed', 'listening', 'listing', 'lists', 'literacy', 'literal', 'literature', 'literatures', 'little', 'live', 'lives', 'living', 'liwc', 'load', 'loading', 'loadings', 'lobata', 'local', 'locality', 'locally', 'locate', 'located', 'location', 'locations', 'locus', 'log', 'logged', 'logging', 'login', 'logistic', 'logitboost', 'logitech', 'logits', 'logs', 'lol', 'long', 'longer', 'longest', 'longterm', 'look', 'looked', 'looking', 'looks', 'loop', 'loops', 'loosely', 'lose', 'losing', 'loss', 'lossless', 'lost', 'lot', 'low', 'lower', 'lowerer', 'lowerperforming', 'lowest', 'lowscoring', 'lsa', 'lstm', 'lstmt', 'lua', 'lunch', 'lution', 'lvl', 'lynch', 'machine', 'machinelearned', 'macro', 'made', 'madrid', 'mae', 'magnitude', 'magnitudes', 'mail', 'main', 'mainly', 'maintain', 'maintained', 'maintaining', 'maintenance', 'major', 'majority', 'majors', 'make', 'makes', 'making', 'male', 'males', 'malkiewich', 'malleability', 'malleable', 'mallick', 'mammals', 'manage', 'management', 'manages', 'managing', 'mance', 'manifest', 'manifested', 'manipulate', 'manipulating', 'manipulation', 'manipulations', 'mann', 'manner', 'manual', 'manually', 'many', 'map', 'mapped', 'mapping', 'maps', 'margin', 'marginal', 'marginally', 'margins', 'markets', 'markov', 'marks', 'markus', 'marx', 'mary', 'masaryk', 'mason', 'mass', 'massive', 'massively', 'master', 'mastered', 'mastering', 'mastery', 'match', 'matched', 'matches', 'matching', 'material', 'materially', 'materials', 'math', 'mathematical', 'mathematics', 'mathspring', 'matmat', 'matrices', 'matrix', 'matter', 'matthews', 'max', 'maximal', 'maximization', 'maximize', 'maximizes', 'maximizing', 'maximum', 'maxintrv', 'maxu', 'may', 'mdp', 'mdps', 'mean', 'meaning', 'meaningful', 'meanings', 'means', 'meansquared', 'measure', 'measured', 'measurement', 'measurements', 'measures', 'measuring', 'mechanical', 'mechanism', 'mechanisms', 'media', 'median', 'medians', 'mediated', 'medical', 'medicine', 'medium', 'mediumsized', 'mello', 'mellon', 'member', 'members', 'memory', 'memphis', 'mental', 'mention', 'mentioned', 'mentioning', 'menu', 'menus', 'mercial', 'merely', 'merged', 'merging', 'mesa', 'message', 'messages', 'messaging', 'met', 'meta', 'metacognitive', 'metadata', 'metatutor', 'method', 'methodical', 'methodological', 'methodologies', 'methodology', 'methods', 'metric', 'metrics', 'metrix', 'metropolitan', 'mftci', 'mftcip', 'mice', 'michener', 'michigan', 'micro', 'microeconomic', 'microorganisms', 'mid', 'middle', 'midwestern', 'might', 'milder', 'miller', 'million', 'milliseconds', 'min', 'minchao', 'mind', 'mindset', 'mine', 'mined', 'mini', 'miniature', 'minima', 'minimal', 'minimally', 'minimization', 'minimize', 'minimized', 'minimizes', 'minimizing', 'minimum', 'mining', 'minnesota', 'minor', 'minority', 'minsplit', 'minute', 'minutes', 'mirrors', 'misaligned', 'miscellaneous', 'misclassified', 'mislabeling', 'misleading', 'miss', 'missing', 'mistake', 'mistakes', 'mitigates', 'mitigating', 'mixed', 'mixing', 'mixture', 'mixtures', 'mlr', 'mobile', 'mobilelearning', 'modal', 'mode', 'model', 'modeled', 'modeler', 'modelfree', 'modeling', 'models', 'moderate', 'modern', 'modes', 'modest', 'modification', 'modifications', 'modified', 'modify', 'modulated', 'module', 'modules', 'molecules', 'mom', 'moments', 'money', 'monitor', 'monitoring', 'monitorrecommend', 'month', 'mooc', 'moocs', 'moreover', 'mortar', 'mostly', 'mostow', 'motion', 'motivate', 'motivated', 'motivating', 'motivation', 'motivational', 'motivations', 'motor', 'mould', 'mouse', 'move', 'moved', 'movement', 'movements', 'moves', 'movie', 'moving', 'msw', 'mtfci', 'mturk', 'much', 'multi', 'multicollinearity', 'multidimensional', 'multimedia', 'multinomial', 'multiple', 'multiplechoice', 'multiplic', 'multiplication', 'multiplied', 'multipliers', 'multivariate', 'multiview', 'muni', 'music', 'musical', 'must', 'mute', 'muth', 'mutual', 'mwixon', 'myopic', 'naive', 'naively', 'name', 'named', 'namely', 'names', 'nanodegrees', 'naomi', 'narcissistic', 'narrative', 'narrativity', 'narrow', 'narrowing', 'narrowly', 'national', 'native', 'natively', 'natural', 'naturalization', 'naturally', 'nature', 'naval', 'navigate', 'navigated', 'navigating', 'navigation', 'na√Øve', 'ncc', 'ncorrectpsstepsincelastwrongkcsession', 'ncsu', 'neal', 'near', 'nearest', 'nearly', 'necessarily', 'necessary', 'necessitates', 'necesssary', 'need', 'needed', 'needs', 'neg', 'negative', 'negatively', 'negatives', 'negativity', 'neglected', 'negligible', 'negligibly', 'neighbor', 'neighborhood', 'neighbors', 'neither', 'nested', 'netflix', 'network', 'networks', 'neural', 'neuropsychological', 'never', 'nevertheless', 'new', 'newer', 'newlevel', 'newline', 'newly', 'news', 'newtonian', 'next', 'ngoc', 'ngram', 'nia', 'nine', 'niter', 'nitera', 'niteruv', 'nlg', 'nlp', 'nmf', 'node', 'nodes', 'noise', 'noisy', 'non', 'noncausal', 'noncontent', 'none', 'nonell', 'nonetheless', 'nonindividualized', 'nonisomorphic', 'nonlinear', 'nonlinearly', 'nonnegative', 'nonnegativity', 'nonnormalized', 'nonpersonalized', 'nontraditional', 'noregret', 'norm', 'normal', 'normalization', 'normalized', 'normally', 'normative', 'north', 'northeastern', 'northstar', 'nose', 'notable', 'notably', 'notation', 'notations', 'note', 'noted', 'notes', 'nothing', 'notice', 'noticed', 'notion', 'notions', 'nouns', 'novel', 'novelty', 'novice', 'novices', 'nrf', 'nrm', 'nsf', 'nstepsincelastwrongkc', 'ntr', 'ntu', 'nuance', 'nuanced', 'nuances', 'nuclear', 'nucleus', 'null', 'number', 'numbers', 'numeric', 'numerical', 'numerous', 'nurse', 'nursing', 'nwesinceps', 'nwestepsincelastwrong', 'nyu', 'oak', 'obfuscating', 'object', 'objective', 'objectives', 'observable', 'observably', 'observation', 'observations', 'observe', 'observed', 'observing', 'obsolete', 'obstacles', 'obtain', 'obtained', 'obtaining', 'obtains', 'obvious', 'obviously', 'occasional', 'occasionally', 'occluded', 'occur', 'occurred', 'occurrence', 'occurrences', 'occurring', 'occurs', 'ochiai', 'octave', 'odd', 'odds', 'oder', 'oentaryo', 'offer', 'offered', 'offering', 'offers', 'office', 'offline', 'offs', 'offset', 'offsets', 'often', 'oftentimes', 'ofthe', 'okay', 'old', 'omit', 'one', 'oneparticipant', 'ones', 'ongoing', 'online', 'onto', 'opacity', 'open', 'opened', 'openended', 'opening', 'operate', 'operated', 'operates', 'operating', 'operation', 'operationalized', 'operations', 'operator', 'operators', 'opinion', 'opinions', 'opportunities', 'opportunity', 'opposed', 'opposite', 'opted', 'optima', 'optimal', 'optimization', 'optimize', 'optimized', 'optimizes', 'option', 'optional', 'options', 'orange', 'order', 'ordered', 'orders', 'org', 'organic', 'organism', 'organisms', 'organization', 'organize', 'organized', 'orientation', 'oriented', 'origin', 'original', 'originally', 'orthographic', 'osg', 'osgood', 'others', 'otherwise', 'outcome', 'outcomes', 'outdoor', 'outdoors', 'outlier', 'outliers', 'outline', 'outlined', 'outlines', 'outlying', 'outperform', 'outperformed', 'outperforming', 'outperforms', 'output', 'outputs', 'outranked', 'outside', 'overall', 'overcome', 'overfit', 'overfitting', 'overlaid', 'overlap', 'overlapped', 'overlapping', 'overlooked', 'overly', 'overor', 'overt', 'overuse', 'overview', 'overwhelmed', 'overwhelming', 'overwhelmingly', 'ownership', 'paced', 'pacific', 'package', 'page', 'pages', 'paid', 'pair', 'paired', 'pairs', 'pairwise', 'pair‚àó', 'palette', 'pam', 'panel', 'paper', 'papers', 'par', 'paradigm', 'paragraphs', 'parallel', 'parameter', 'parameters', 'parametric', 'parametrization', 'parametrize', 'parametrized', 'paramount', 'parent', 'parentheses', 'parenthesis', 'parenthesized', 'parents', 'parikh', 'paris', 'parisian', 'parlance', 'part', 'partial', 'partially', 'participant', 'participants', 'participate', 'participated', 'participating', 'participation', 'particular', 'particularly', 'partition', 'partitioned', 'partitions', 'partly', 'partner', 'parts', 'pas', 'pass', 'passed', 'passing', 'passion', 'passive', 'passive‚Äìactive', 'password', 'past', 'patent', 'path', 'paths', 'pathways', 'patient', 'patients', 'pattern', 'patterns', 'pause', 'paused', 'pauses', 'pausing', 'paying', 'payoffs', 'pca', 'pct', 'pdb', 'pdf', 'pearson', 'pedagogical', 'pedagogies', 'pedagogy', 'peer', 'peers', 'pekrun', 'pelanek', 'peleato', 'pel√°nek', 'penalize', 'penalizing', 'peng', 'pennebaker', 'people', 'per', 'perceived', 'percentage', 'percentages', 'perception', 'perceptions', 'perceptual', 'perfect', 'perfetti', 'perfor', 'perforamance', 'perfordents', 'perform', 'performance', 'performanceapproach', 'performances', 'performed', 'performers', 'performing', 'performs', 'perform‚Ä¶', 'perhaps', 'period', 'periodic', 'periodically', 'periods', 'perseverance', 'persist', 'persistence', 'person', 'personal', 'personalgrouped', 'personality', 'personalized', 'personalizes', 'personalizing', 'personnel', 'perspective', 'perspectives', 'pertained', 'pertaining', 'pervasiveness', 'pessimism', 'pete', 'peter', 'pham', 'phase', 'phases', 'phd', 'phenomena', 'phenomenon', 'phi', 'philosophy', 'phone', 'phones', 'photoplethysmography', 'phrase', 'phrases', 'physical', 'physics', 'physiological', 'physiology', 'picked', 'picking', 'picture', 'piece', 'piech', 'pipeline', 'pis', 'pitfall', 'pittsburgh', 'pixel', 'pixels', 'place', 'placed', 'placements', 'plan', 'plane', 'planner', 'planners', 'planning', 'plans', 'plant', 'plants', 'plass', 'platform', 'platforms', 'play', 'playback', 'played', 'player', 'players', 'playing', 'playlists', 'plays', 'playtracer', 'pleased', 'plenty', 'plethora', 'plot', 'plots', 'plotted', 'point', 'pointed', 'points', 'poised', 'poles', 'polices', 'policies', 'policy', 'polygon', 'polynomial', 'polytechnique', 'polyzou', 'pontificate', 'pool', 'pooling', 'poor', 'poorly', 'popular', 'popularity', 'population', 'populations', 'portion', 'portions', 'portuguese', 'pose', 'posed', 'poses', 'posing', 'positing', 'position', 'positioned', 'positive', 'positively', 'positives', 'possessing', 'possibilities', 'possibility', 'possible', 'possibly', 'post', 'poster', 'posterior', 'postpruning', 'posts', 'posttest', 'posttests', 'potential', 'potentially', 'poverty', 'power', 'powerful', 'ppg', 'practical', 'practically', 'practice', 'practiced', 'practices', 'practicing', 'pragmatic', 'praised', 'pre', 'preand', 'preceding', 'precious', 'precise', 'precisely', 'precision', 'predecessor', 'predefined', 'predetermined', 'predicated', 'prediciton', 'predict', 'predictable', 'predicted', 'predicting', 'prediction', 'predictions', 'predictive', 'predictor', 'predictors', 'predicts', 'predispositions', 'preemptive', 'prefer', 'preference', 'preferences', 'preliminaries', 'preliminary', 'prematurely', 'preparation', 'preparations', 'prepare', 'prepared', 'preparing', 'prepost', 'prerequisite', 'prerequisites', 'prescriptive', 'presence', 'present', 'presentation', 'presented', 'presenting', 'presently', 'presents', 'preserved', 'preserving', 'press', 'pressing', 'presumably', 'pretest', 'pretests', 'prevalence', 'prevent', 'preventing', 'prevents', 'previous', 'previously', 'prey', 'pride', 'primarily', 'primary', 'principal', 'principle', 'principled', 'principles', 'print', 'prior', 'priori', 'prioritized', 'priority', 'priors', 'privacy', 'private', 'pro', 'proactively', 'probabilistic', 'probabilistically', 'probabilities', 'probability', 'probably', 'probdiff', 'probe', 'probes', 'probity', 'problem', 'problematic', 'problemj', 'problems', 'problemsolving', 'procedure', 'procedures', 'proceed', 'proceeded', 'proceedings', 'proceeds', 'process', 'processed', 'processes', 'processing', 'produce', 'produced', 'produces', 'producing', 'product', 'production', 'productive', 'productivity', 'products', 'professional', 'proficiency', 'profiles', 'program', 'programmatically', 'programming', 'programs', 'progress', 'progresses', 'progressing', 'progression', 'progressions', 'prohibitive', 'project', 'projected', 'projecting', 'projection', 'prominent', 'promise', 'promising', 'promote', 'promotes', 'promoting', 'prompt', 'prompted', 'prompting', 'prompts', 'prone', 'pronounced', 'proof', 'propagate', 'propagation', 'properly', 'properties', 'property', 'proportion', 'proportional', 'proportionalized', 'proportions', 'proposals', 'propose', 'proposed', 'prospects', 'protein', 'proteins', 'protocol', 'protocols', 'proved', 'proves', 'provide', 'provided', 'provides', 'providing', 'provision', 'proximity', 'proxy', 'prudent', 'prune', 'pruned', 'pruning', 'pscount', 'pseudo', 'psyc', 'psycholinguistic', 'psychologist', 'psychology', 'psychometric', 'pta', 'public', 'publication', 'publicly', 'published', 'publishing', 'pull', 'punctuation', 'pupil', 'pupillometry', 'purchase', 'purely', 'purple', 'purpose', 'purposes', 'pursue', 'pursued', 'pursuing', 'push', 'pushing', 'put', 'putting', 'puzzle', 'puzzles', 'pyramid', 'pyrenees', 'qian', 'qmatrix', 'qualified', 'quality', 'quantification', 'quantified', 'quantify', 'quantifying', 'quantile', 'quantitative', 'quantities', 'quantity', 'quartiles', 'quercus', 'queried', 'query', 'question', 'questionable', 'questionnaires', 'questions', 'question‚Äìconcept', 'quickly', 'quiet', 'quintessential', 'quit', 'quite', 'quiz', 'quizzes', 'race', 'radar', 'radek', 'radial', 'raise', 'raiser', 'raises', 'raising', 'ran', 'rand', 'random', 'randomized', 'randomly', 'randomness', 'range', 'ranged', 'ranges', 'ranging', 'rank', 'ranked', 'rankings', 'ranks', 'ranksum', 'rapid', 'rapidly', 'rapport', 'rare', 'rarely', 'rasch', 'rate', 'ratechange', 'rated', 'rater', 'raters', 'rates', 'rather', 'rating', 'ratings', 'ratio', 'ration', 'ratios', 'raw', 'rbf', 'reach', 'reached', 'reaches', 'reaching', 'reaction', 'reactive', 'read', 'readable', 'reader', 'readers', 'readily', 'reading', 'readings', 'ready', 'real', 'realistic', 'reality', 'realization', 'realize', 'realized', 'realizing', 'realm', 'realtime', 'realworld', 'reason', 'reasonable', 'reasonably', 'reasoning', 'reasons', 'recall', 'receipt', 'receive', 'received', 'receiver', 'receives', 'receiving', 'recent', 'recently', 'reception', 'recipe', 'recipes', 'recoded', 'recognition', 'recognize', 'recognized', 'recommend', 'recommendation', 'recommendations', 'recommended', 'recommender', 'recommending', 'recommends', 'reconstruct', 'reconstructed', 'reconstruction', 'reconstructs', 'record', 'recorded', 'recording', 'recordings', 'records', 'recruited', 'rectified', 'recurrent', 'recursion', 'recursive', 'recursively', 'recycle', 'recycling', 'red', 'redesigned', 'redirect', 'redirecting', 'reduce', 'reduced', 'reduces', 'reducing', 'reduction', 'redundancy', 'reengaging', 'refer', 'referenced', 'references', 'referential', 'referred', 'refers', 'refine', 'refined', 'refinement', 'refining', 'reflect', 'reflected', 'reflection', 'reflective', 'reflects', 'reformulating', 'refresh', 'refreshing', 'regard', 'regarding', 'regardless', 'regimented', 'regions', 'register', 'registered', 'regression', 'regret', 'regular', 'regularities', 'regularization', 'regularized', 'regularizes', 'regularly', 'regulate', 'regulated', 'regulating', 'regulation', 'regulatory', 'reinchard', 'reinforcement', 'rejected', 'rejection', 'relate', 'related', 'relates', 'relation', 'relations', 'relationship', 'relationships', 'relative', 'relatively', 'relaxing', 'relevance', 'relevant', 'reliability', 'reliable', 'reliably', 'reliance', 'relied', 'relief', 'relieff', 'relies', 'reloads', 'relu', 'rely', 'relying', 'remain', 'remainder', 'remained', 'remaining', 'remains', 'remarks', 'remedial', 'remediate', 'remember', 'remembering', 'remote', 'removal', 'remove', 'removed', 'removes', 'removing', 'render', 'rendered', 'renders', 'reorient', 'reorienting', 'repeat', 'repeated', 'repeatedly', 'repeating', 'repeats', 'repetition', 'repetitive', 'replace', 'replaced', 'replacing', 'replay', 'replayability', 'replayed', 'replaying', 'replays', 'replicate', 'replication', 'replications', 'replies', 'reply', 'report', 'reported', 'reporting', 'reports', 'repository', 'represent', 'representation', 'representational', 'representations', 'representative', 'representatives', 'represented', 'representing', 'represents', 'reproducible', 'republic', 'request', 'requested', 'requesting', 'require', 'required', 'requirement', 'requirements', 'requires', 'requiring', 'requisite', 'reread', 'rereading', 'resampling', 'research', 'researched', 'researchers', 'reshape', 'reshness', 'reside', 'resolve', 'resources', 'resp', 'respect', 'respective', 'respectively', 'respects', 'respond', 'responded', 'respondents', 'responding', 'response', 'responses', 'responsetime', 'responsible', 'rest', 'restart', 'restore', 'restrict', 'restricted', 'restriction', 'result', 'resultant', 'resulted', 'resulting', 'results', 'retain', 'retained', 'retention', 'retrain', 'retrained', 'retraining', 'retrieval', 'returned', 'returns', 'reuse', 'revamp', 'reveal', 'revealed', 'reveals', 'review', 'reviewed', 'reviewer', 'reviewers', 'reviewing', 'reviews', 'revise', 'revised', 'revising', 'revision', 'revisit', 'revisited', 'reward', 'rewards', 'rewinding', 'rewinds', 'rich', 'richard', 'richer', 'richly', 'right', 'rightapp', 'risk', 'risks', 'rlinduced', 'rmse', 'rnn', 'robust', 'robustness', 'roc', 'rock', 'roda', 'role', 'roles', 'roll', 'rong', 'room', 'rooms', 'root', 'rooted', 'rosettacommons', 'roughly', 'rounds', 'route', 'routine', 'routines', 'row', 'rowe', 'rows', 'rpart', 'rsjd', 'rubber', 'rubric', 'rubrics', 'rui', 'rule', 'rules', 'run', 'running', 'runs', 'rupted', 'rush', 'rushing', 'rutherford', 'sabourin', 'saccade', 'saccades', 'sae', 'said', 'salient', 'sam', 'sample', 'sampled', 'samples', 'sampling', 'sat', 'satisfactory', 'satisfy', 'save', 'saving', 'savings', 'saw', 'say', 'sbe', 'scaffold', 'scaffolded', 'scaffolding', 'scaffolds', 'scala', 'scalability', 'scalable', 'scalar', 'scale', 'scales', 'scaling', 'scanning', 'scenario', 'scenarios', 'schedule', 'scheme', 'scholand', 'school', 'schools', 'sci', 'science', 'sciences', 'scientific', 'scikit', 'scope', 'scopes', 'score', 'scorebased', 'scored', 'scores', 'scoring', 'scott', 'scratched', 'screen', 'screens', 'screenshot', 'scripts', 'scroll', 'scrolling', 'sdk', 'seamless', 'search', 'searching', 'second', 'secondarily', 'secondary', 'seconds', 'section', 'sections', 'secure', 'security', 'see', 'seeing', 'seek', 'seeking', 'seeks', 'seem', 'seemed', 'seems', 'seen', 'segment', 'segmentation', 'segmenting', 'segments', 'select', 'selected', 'selecting', 'selection', 'selections', 'selective', 'selects', 'self', 'selfgenerated', 'selfregulated', 'selfreported', 'selfreports', 'selves', 'semantic', 'semantically', 'semantics', 'semester', 'semesters', 'semi', 'semisupervised', 'send', 'sens', 'sense', 'sensed', 'sensible', 'sensitive', 'sensitivity', 'sensors', 'sentence', 'sentences', 'sentiment', 'separable', 'separate', 'separated', 'separately', 'separates', 'separating', 'sequence', 'sequences', 'sequencing', 'sequential', 'sequentially', 'series', 'serious', 'serve', 'served', 'serves', 'service', 'services', 'serving', 'session', 'sessions', 'set', 'sets', 'setting', 'settings', 'setup', 'seven', 'seventh', 'several', 'severe', 'sex', 'sexual', 'shaffer', 'shall', 'shallow', 'shame', 'shape', 'share', 'shared', 'shares', 'sharing', 'sharp', 'sharply', 'shed', 'sheets', 'shelf', 'shen', 'shift', 'shifts', 'shore', 'short', 'shortcomings', 'shortened', 'shorter', 'shortterm', 'shot', 'shotterm', 'show', 'showed', 'showing', 'shown', 'shows', 'shrinkage', 'shrinks', 'shung', 'sick', 'side', 'sidered', 'sides', 'sigma', 'sigmoid', 'sign', 'signal', 'signals', 'significance', 'significances', 'significant', 'significantly', 'signify', 'signing', 'sij', 'silently', 'sim', 'simi', 'similar', 'similarities', 'similarity', 'similarly', 'simon', 'simple', 'simpler', 'simplest', 'simplicity', 'simplified', 'simplify', 'simplifying', 'simply', 'simulate', 'simulated', 'simulation', 'simulations', 'simultaneously', 'since', 'sine', 'singapore', 'single', 'singular', 'sit', 'situate', 'situation', 'situations', 'six', 'sixty', 'sizable', 'size', 'sized', 'sizefits', 'sizes', 'skew', 'skewed', 'skill', 'skilled', 'skillful', 'skilli', 'skills', 'skillÃÇ', 'skimming', 'skip', 'skipped', 'skipping', 'skive', 'slfn', 'slfns', 'slider', 'sliding', 'slight', 'slightly', 'slip', 'slipping', 'slna', 'slope', 'slopes', 'slow', 'slowly', 'small', 'smaller', 'smallest', 'smarter', 'smartphone', 'smartphones', 'smi', 'smoothed', 'smote', 'sna', 'snapshot', 'snapshots', 'snb', 'snippet', 'soap', 'social', 'socially', 'society', 'socio', 'sociology', 'soft', 'software', 'sokal', 'solely', 'solid', 'solo', 'solution', 'solutions', 'solve', 'solved', 'solver', 'solvers', 'solving', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'soon', 'sooo', 'sophisticated', 'sophistication', 'sorted', 'sorting', 'sosg', 'sought', 'sounded', 'sounds', 'source', 'sources', 'south', 'southern', 'space', 'spaces', 'spacing', 'spain', 'span', 'spanning', 'sparfa', 'sparse', 'sparsely', 'sparsity', 'spatial', 'spawned', 'speaking', 'spearman', 'special', 'specialization', 'specialized', 'species', 'specific', 'specifically', 'specificity', 'specified', 'specifies', 'specify', 'speech', 'speed', 'speedily', 'speeding', 'spelling', 'spend', 'spending', 'spent', 'sperm', 'spinners', 'spinning', 'split', 'spliting', 'splits', 'spoken', 'sponses', 'spreads', 'spring', 'springer', 'square', 'squared', 'squares', 'srl', 'ssae', 'stability', 'stable', 'staff', 'stage', 'stages', 'stakes', 'stall', 'stand', 'standard', 'standardized', 'standards', 'stands', 'stanford', 'stanza', 'star', 'start', 'started', 'starting', 'starts', 'state', 'stated', 'statement', 'statements', 'states', 'static', 'stationary', 'statistic', 'statistical', 'statistically', 'statistics', 'stats', 'status', 'stay', 'stayed', 'staying', 'stays', 'std', 'stem', 'stemmed', 'stems', 'step', 'stephen', 'steps', 'stifle', 'still', 'stimuli', 'stimulus', 'stimulusindependent', 'stmath', 'stochastic', 'stochastically', 'stop', 'stopped', 'stops', 'stopwords', 'store', 'stored', 'stores', 'stories', 'story', 'straightforward', 'strategic', 'strategies', 'strategizer', 'strategy', 'stratified', 'stream', 'streaming', 'strength', 'strengthened', 'strengths', 'stress', 'strictly', 'strike', 'striking', 'striving', 'strong', 'stronger', 'strongest', 'strongly', 'structural', 'structure', 'structured', 'structuredness', 'structures', 'struggled', 'struggling', 'stu', 'stuck', 'student', 'studentindependent', 'studentinitiated', 'studentobjective', 'studentparameter', 'students', 'studentsaÃÅ', 'studham', 'studied', 'studies', 'studio', 'study', 'studying', 'style', 'styles', 'sub', 'subgoal', 'subgoals', 'subgroups', 'subject', 'subjective', 'subjects', 'submission', 'submits', 'submitted', 'suboptimal', 'subsequent', 'subsequently', 'subset', 'subsets', 'substantially', 'substantive', 'substantively', 'substituting', 'subtitles', 'subtle', 'subtracting', 'subtraction', 'subtractionlevel', 'subtype', 'succ', 'succeed', 'succeeded', 'success', 'successful', 'successfully', 'successively', 'suffer', 'suffering', 'sufficient', 'sufficiently', 'suffix', 'suffixes', 'suggest', 'suggested', 'suggesting', 'suggestion', 'suggestive', 'suggests', 'suitable', 'suite', 'suited', 'sum', 'summaries', 'summarization', 'summarize', 'summarized', 'summarizes', 'summarizing', 'summary', 'summation', 'summative', 'summatively', 'summed', 'sums', 'sun', 'superior', 'superiority', 'supersede', 'superseded', 'supervised', 'supplement', 'supplemental', 'supplementary', 'support', 'supported', 'supporting', 'supports', 'supposed', 'suppression', 'supscriptt', 'sure', 'surface', 'surgical', 'surpass', 'surprising', 'surprisingly', 'surrounding', 'survey', 'surveys', 'svd', 'svm', 'svms', 'swapping', 'sweeney', 'swiping', 'swiss', 'switzerland', 'symbols', 'symmetrically', 'synchronized', 'synchronously', 'synonym', 'synonymselection', 'syntactic', 'syntactical', 'syntax', 'synthesis', 'synthesize', 'synthetic', 'system', 'systematic', 'systematically', 'systems', 'table', 'tables', 'tablet', 'tablets', 'tackle', 'tactic', 'tag', 'tagging', 'tail', 'tailed', 'tailor', 'tailored', 'tailormade', 'take', 'taken', 'takes', 'taking', 'talk', 'talked', 'tall', 'tanh', 'tanja', 'tapping', 'taps', 'target', 'targeted', 'targeting', 'targets', 'taruther', 'tasa', 'task', 'taskbar', 'tasked', 'tasks', 'tat', 'taught', 'taxonomy', 'teach', 'teacher', 'teachers', 'teaches', 'teaching', 'team', 'teams', 'technical', 'technique', 'techniques', 'technologies', 'technologists', 'technology', 'tells', 'temporal', 'tempt', 'tempts', 'ten', 'tencrossing', 'tend', 'tended', 'tendency', 'tends', 'tension', 'tensor', 'tensorflowtm', 'teomara', 'terface', 'term', 'terminate', 'terminates', 'terminology', 'terms', 'tertiary', 'test', 'testbed', 'tested', 'testing', 'tests', 'text', 'textbook', 'textbooks', 'texts', 'textual', 'thank', 'thanks', 'theart', 'theater', 'theme', 'themes', 'theorem', 'theoretical', 'theoretically', 'theories', 'theory', 'thereby', 'therefore', 'thesis', 'thesisÃÅcole', 'thin', 'thing', 'things', 'think', 'thinking', 'third', 'thirds', 'thomas', 'thorough', 'thoroughly', 'though', 'thought', 'thoughts', 'thousand', 'thousands', 'thread', 'threads', 'threaten', 'threatened', 'three', 'threshold', 'thresholding', 'thresholds', 'throughout', 'thus', 'tick', 'ticks', 'tie', 'tied', 'tier', 'tiffany', 'tight', 'tightener', 'time', 'timeinsession', 'timeliness', 'timely', 'timer', 'times', 'timescale', 'timestamp', 'timesvd', 'timevarying', 'timing', 'timperley', 'tinuous', 'tion', 'titles', 'tkaeser', 'tobii', 'today', 'together', 'tokens', 'told', 'tolerance', 'tony', 'took', 'tool', 'toolkit', 'tools', 'top', 'topic', 'topics', 'toronto', 'tors', 'tospeech', 'total', 'totalpstime', 'touchstone', 'touted', 'toward', 'towards', 'trace', 'tracing', 'track', 'tracked', 'tracker', 'trackers', 'tracking', 'tracks', 'tractable', 'trade', 'tradeoff', 'traditional', 'traditionally', 'train', 'trained', 'training', 'traitbased', 'traits', 'trajectories', 'trajectory', 'transductive', 'transfer', 'transferred', 'transferring', 'transform', 'transformation', 'transformations', 'transformed', 'transforming', 'transforms', 'transiting', 'transition', 'transitioning', 'transitions', 'translates', 'translating', 'translations', 'transparent', 'transportation', 'transpose', 'transversal', 'travel', 'treat', 'treated', 'treating', 'treatment', 'treats', 'tree', 'trees', 'trend', 'trends', 'trendy', 'trg', 'tri', 'trial', 'trials', 'triangle', 'triangles', 'tribe', 'tried', 'trigger', 'triggered', 'triggers', 'trims', 'trip', 'triples', 'trips', 'tristan', 'trivial', 'true', 'trust', 'truth', 'try', 'trying', 'tsai', 'tth', 'tune', 'tuned', 'tuning', 'tuple', 'turk', 'turn', 'turned', 'turning', 'turns', 'tutor', 'tutorial', 'tutoring', 'tutors', 'tuulos', 'twenty', 'twice', 'two', 'twostate', 'type', 'typed', 'typeface', 'types', 'typical', 'typically', 'typing', 't√≥th', 'ubiquitous', 'ucb', 'udacity', 'ultimate', 'ultimately', 'umass', 'umimecesky', 'umn', 'umƒ±Ãåesky', 'unable', 'unaltered', 'unbiased', 'uncertainty', 'uncle', 'unclear', 'uncovered', 'underdetermined', 'undergoing', 'undergraduate', 'undergraduates', 'underlined', 'underlying', 'undersampled', 'underscoring', 'understand', 'understandable', 'understanding', 'understood', 'undertook', 'underwent', 'undoes', 'unfamiliar', 'unfolds', 'unfortunately', 'unicode', 'unified', 'uniformly', 'unilateral', 'uninformative', 'unintended', 'uninteresting', 'uninterrupted', 'unique', 'unit', 'united', 'units', 'univariate', 'universal', 'universally', 'universities', 'university', 'unix', 'unknown', 'unlabeled', 'unless', 'unlike', 'unnecessary', 'unobserved', 'unrealistic', 'unrelated', 'unreliable', 'unseen', 'unsolved', 'unsuitable', 'unsupervised', 'unsure', 'unsurprising', 'untaken', 'untrained', 'unusual', 'unweighted', 'upcoming', 'update', 'updated', 'updates', 'updating', 'upmost', 'upon', 'upper', 'upvote', 'urban', 'url', 'usa', 'usable', 'usage', 'use', 'used', 'useful', 'usefulness', 'user', 'username', 'users', 'uses', 'using', 'usual', 'usually', 'utility', 'utilization', 'utilize', 'utilized', 'utilizing', 'uts', 'utterance', 'utterances', 'utters', 'vae', 'vaes', 'vague', 'valence', 'valid', 'validate', 'validated', 'validates', 'validating', 'validation', 'validity', 'valuable', 'value', 'valued', 'values', 'vanishingly', 'variability', 'variable', 'variableorder', 'variables', 'variance', 'variances', 'variants', 'variational', 'variations', 'varied', 'varies', 'variety', 'various', 'vary', 'varying', 'vasile', 'vast', 'vayssiers', 'vec', 'vector', 'vectorization', 'vectorized', 'vectors', 'vegetation', 'velocity', 'verbal', 'verbatim', 'verbs', 'verify', 'verifying', 'versa', 'version', 'versions', 'versus', 'vertical', 'via', 'viability', 'vice', 'vicious', 'vide', 'video', 'videos', 'videowatching', 'view', 'viewed', 'viewing', 'views', 'vigilance', 'violated', 'virtual', 'visible', 'vision', 'visit', 'visited', 'visiting', 'visual', 'visualization', 'visualizationbased', 'visualizations', 'visualize', 'visualized', 'visualizes', 'visualizing', 'visually', 'vital', 'vmm', 'vmms', 'vocabularies', 'vocabulary', 'voice', 'voltage', 'volume', 'volumes', 'voluntary', 'vulnerable', 'waiting', 'waitpages', 'walk', 'wallner', 'wander', 'wandering', 'wanes', 'wang', 'waning', 'want', 'wanted', 'ward', 'warm', 'warning', 'warranted', 'warrants', 'washington', 'watch', 'watched', 'watches', 'watching', 'way', 'ways', 'weak', 'weaker', 'weakness', 'wears', 'web', 'webb', 'webcam', 'webcams', 'webmail', 'website', 'websites', 'week', 'weeks', 'weight', 'weightage', 'weightages', 'weighted', 'weighting', 'weightings', 'weights', 'weka', 'well', 'wellknown', 'weren', 'western', 'whatever', 'wheel', 'whenever', 'whereas', 'whereby', 'wherever', 'whether', 'whitney', 'whole', 'whose', 'wide', 'widely', 'wider', 'width', 'wiggle', 'willing', 'window', 'windowed', 'windows', 'winslow', 'winsorization', 'wise', 'wish', 'within', 'withinand', 'withincluster', 'withintutor', 'without', 'wixon', 'women', 'won', 'wondered', 'woody', 'word', 'wording', 'wordings', 'wordnet', 'words', 'work', 'worked', 'worker', 'workers', 'workhorses', 'working', 'works', 'workstations', 'world', 'worse', 'worst', 'worth', 'would', 'wpi', 'wrinkle', 'wrinkler', 'write', 'writing', 'writng', 'written', 'wrong', 'www', 'wxy', 'w¬µz', 'wœÉz', 'xnt', 'xue', 'yeah', 'year', 'years', 'yellow', 'yes', 'yet', 'yield', 'yielded', 'yielding', 'yields', 'yoked', 'yokedcontrol', 'york', 'young', 'yule', 'yuntao', 'zero', 'zeros', 'zhang', 'zhu', 'zoning', 'ztransform', 'zurich', '¬±', '¬µŒ∏t', '¬∑', '√ó', '√©cole', 'Àú', 'Œ±jt', 'Œ±xt', 'Œ∏lj', 'Œªkak', 'Œªkz', 'œÉŒ∏t', '‚Äì', '‚Äî', '‚Äòamount', '‚Äòhomework', '‚Äòlarge', '‚Äòlives', '‚Ä¢', '‚Üë', '‚Üí', '‚àÜ', '‚àà', '‚àó', '‚àû', '‚â§', '‚â•', '‚â∫', '‚óè', '‚óè‚óè', '\\uf0b7', 'ùêºùê∑ùêπ', 'ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ', 'ùëÖùëíùëêùëéùëôùëô', 'ùëáùêπùëñ', 'ùëîùëéùëñùëõ', 'ùëöùëíùëéùë†ùë¢ùëüùëí', 'ùëùùëúùë†ùë°ùë°ùëíùë†ùë°', 'ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ', 'ùëùùëüùëíùë°ùëíùë†ùë°', 'ùëüùëíùëêùëéùëôùëô', 'ùë†ùëêùëúùëüùëí']\n"
     ]
    }
   ],
   "source": [
    "# create a function that takes in a list of documents\n",
    "# and returns a set of unique words. Make sure that you\n",
    "# sort the list alphabetically before returning it. \n",
    "\n",
    "def get_vocabulary(docs):\n",
    "    voc = []\n",
    "    for doc in docs:\n",
    "        for word in doc.split():\n",
    "            if word not in voc: \n",
    "                voc.append(word)\n",
    "    voc = list(set(voc))\n",
    "    voc.sort()\n",
    "    return voc\n",
    "\n",
    "# Then print the length of your vocabulary (it should be \n",
    "# around 5500 words)\n",
    "vocabulary = get_vocabulary(docs)\n",
    "print(len(vocabulary))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - transform your documents in to 100-words chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that takes in a list of documents\n",
    "# and returns a list of 100-words chunk \n",
    "# (with a 25 words overlap between them)\n",
    "# Optional: add two arguments, one for the number of words\n",
    "# in each chunk, and one for the overlap\n",
    "\n",
    "def flatten_and_overlap(docs, window_size=100, overlap=25):\n",
    "    \n",
    "    # create the list of overlapping documents\n",
    "    new_list_of_documents = []\n",
    "    \n",
    "    # flatten everything into one string\n",
    "    flat = \"\"\n",
    "    for doc in docs:\n",
    "        flat += doc\n",
    "    \n",
    "    # split into words\n",
    "    flat = flat.split()\n",
    "\n",
    "    # create chunks of 100 words\n",
    "    high = window_size\n",
    "    while high < len(flat):\n",
    "        low = high - window_size\n",
    "        new_list_of_documents.append(flat[low:high])\n",
    "        high += overlap\n",
    "    return new_list_of_documents\n",
    "\n",
    "chunks = flatten_and_overlap(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a for loop to double check that each chunk has \n",
    "# a length of 100\n",
    "# Optional: use assert to do this check\n",
    "for chunk in chunks: \n",
    "    assert(len(chunk) == 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 6 - VECTOR MANIPULATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Create a word by document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>academic</th>\n",
       "      <th>...</th>\n",
       "      <th>ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ</th>\n",
       "      <th>ùëÖùëíùëêùëéùëôùëô</th>\n",
       "      <th>ùëáùêπùëñ</th>\n",
       "      <th>ùëîùëéùëñùëõ</th>\n",
       "      <th>ùëöùëíùëéùë†ùë¢ùëüùëí</th>\n",
       "      <th>ùëùùëúùë†ùë°ùë°ùëíùë†ùë°</th>\n",
       "      <th>ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ</th>\n",
       "      <th>ùëùùëüùëíùë°ùëíùë†ùë°</th>\n",
       "      <th>ùëüùëíùëêùëéùëôùëô</th>\n",
       "      <th>ùë†ùëêùëúùëüùëí</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 5648 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abilities  ability  able  abnormal  absence  absent  absolute  abstract  \\\n",
       "0          0        0     0         0        0       0         0         0   \n",
       "1          0        0     0         0        0       0         0         0   \n",
       "2          0        0     0         0        0       0         0         0   \n",
       "3          0        0     0         0        0       0         0         0   \n",
       "4          0        0     0         0        0       0         0         0   \n",
       "\n",
       "   abstracts  academic  ...  ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ  ùëÖùëíùëêùëéùëôùëô  ùëáùêπùëñ  ùëîùëéùëñùëõ  ùëöùëíùëéùë†ùë¢ùëüùëí  ùëùùëúùë†ùë°ùë°ùëíùë†ùë°  \\\n",
       "0          0         0  ...          0       0    0     0        0         0   \n",
       "1          0         0  ...          0       0    0     0        0         0   \n",
       "2          0         0  ...          0       0    0     0        0         0   \n",
       "3          0         0  ...          0       0    0     0        0         0   \n",
       "4          0         0  ...          0       0    0     0        0         0   \n",
       "\n",
       "   ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ  ùëùùëüùëíùë°ùëíùë†ùë°  ùëüùëíùëêùëéùëôùëô  ùë†ùëêùëúùëüùëí  \n",
       "0          0        0       0      0  \n",
       "1          0        0       0      0  \n",
       "2          0        0       0      0  \n",
       "3          0        0       0      0  \n",
       "4          0        0       0      0  \n",
       "\n",
       "[5 rows x 5648 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) create an empty dataframe using pandas\n",
    "# the number of rows should be the number of documents we have\n",
    "# the number of columns should be size of the vocabulary\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(0, index=np.arange(len(chunks)), columns=vocabulary)\n",
    "df.head()\n",
    "#df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 "
     ]
    }
   ],
   "source": [
    "# 2) fill out the dataframe with the count of words for each chunk\n",
    "# (use two for loops to iterate through the chunks and the vocabulary)\n",
    "for i, chunk in enumerate(chunks):         \n",
    "    for word in chunk:\n",
    "        if word in df.columns:\n",
    "            df.loc[i, word] += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abstract', 'study', 'investigates', 'possible', 'way', 'analyze', 'chat', 'data', 'collaborative', 'learning', 'environments', 'using', 'epistemic', 'network', 'analysis', 'topic', 'modeling', 'topic', 'general', 'topic', 'model', 'built', 'tasa', 'touchstone', 'applied', 'science', 'associates', 'corpus', 'used', 'study', 'topic', 'scores', 'utterances', 'chat', 'data', 'computed', 'seven', 'relevant', 'topics', 'selected', 'based', 'total', 'document', 'scores', 'aggregated', 'topic', 'scores', 'power', 'predicting', 'students', 'learning', 'using', 'epistemic', 'network', 'analysis', 'enables', 'assessing', 'data', 'different', 'angle', 'results', 'showed', 'topic', 'score', 'based', 'epistemic', 'networks', 'low', 'gain', 'students', 'high', 'gain', 'students', 'significantly', 'different', 'overall', 'results', 'suggest', 'two', 'analytical', 'approaches', 'provide', 'complementary', 'information', 'afford', 'new', 'insights', 'processes', 'related', 'successful', 'collaborative', 'interactions', 'keywords', 'chat', 'collaborative', 'learning', 'topic', 'modeling', 'epistemic', 'network']\n",
      "   abilities  ability  able  abnormal  absence  absent  absolute  abstract  \\\n",
      "0          0        0     0         0        0       0         0         1   \n",
      "1          0        0     0         0        0       0         0         0   \n",
      "2          0        0     0         0        0       0         0         0   \n",
      "3          0        0     0         0        0       0         0         0   \n",
      "4          0        0     0         0        0       0         0         0   \n",
      "\n",
      "   abstracts  academic  ...  ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ  ùëÖùëíùëêùëéùëôùëô  ùëáùêπùëñ  ùëîùëéùëñùëõ  ùëöùëíùëéùë†ùë¢ùëüùëí  ùëùùëúùë†ùë°ùë°ùëíùë†ùë°  \\\n",
      "0          0         0  ...          0       0    0     0        0         0   \n",
      "1          0         0  ...          0       0    0     0        0         0   \n",
      "2          0         0  ...          0       0    0     0        0         0   \n",
      "3          0         0  ...          0       0    0     0        0         0   \n",
      "4          0         0  ...          0       0    0     0        0         0   \n",
      "\n",
      "   ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ  ùëùùëüùëíùë°ùëíùë†ùë°  ùëüùëíùëêùëéùëôùëô  ùë†ùëêùëúùëüùëí  \n",
      "0          0        0       0      0  \n",
      "1          0        0       0      0  \n",
      "2          0        0       0      0  \n",
      "3          0        0       0      0  \n",
      "4          0        0       0      0  \n",
      "\n",
      "[5 rows x 5648 columns]\n"
     ]
    }
   ],
   "source": [
    "# 3) Sanity check: make sure that your counts are correct\n",
    "# (e.g., if you know that a words appears often in a document, check that\n",
    "# the number is also high in your dataframe; and vice-versa for low counts)\n",
    "# print(chunks)\n",
    "print(chunks[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 2000 2100 2200 "
     ]
    }
   ],
   "source": [
    "# 4) Putting it together: create a function that takes a list of documents\n",
    "# and a vocabulary as arguments, and returns a dataframe with the counts\n",
    "# of words: \n",
    "\n",
    "# ensure that documents are clean before calling this function\n",
    "def create_matrix(documents, vocabulary):\n",
    "    # initializes dataframe\n",
    "    chunks = flatten_and_overlap(documents)\n",
    "    df_matrix = pd.DataFrame(0, index=np.arange(len(chunks)), columns=vocabulary)\n",
    "    # fills dataframe\n",
    "    for i, chunk in enumerate(chunks):         \n",
    "        for word in chunk:\n",
    "            if word in df_matrix.columns:\n",
    "                df_matrix.loc[i, word] += 1\n",
    "        if i % 100 == 0:\n",
    "            print(i, end=' ')\n",
    "    return df_matrix\n",
    "    \n",
    "\n",
    "# call the function and check that the resulting dataframe is correct\n",
    "df_new_matrix = create_matrix(docs, vocabulary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Weight word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) create a function that adds one to the current cell and takes its log\n",
    "# IF the value in the cell is not zero\n",
    "\n",
    "import math\n",
    "\n",
    "def weight_word_freq(cell):\n",
    "    if cell != 0:\n",
    "        cell = 1 + math.log(cell)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Unnamed: 1  Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5  \\\n",
      "0           0           0           0           0           0           0   \n",
      "1           1           0           0           0           0           0   \n",
      "2           2           0           0           0           0           0   \n",
      "3           3           0           0           0           0           0   \n",
      "4           4           0           0           0           0           0   \n",
      "\n",
      "   Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  ...  ùëÖùëíùëêùëéùëôùëô  ùëáùêπùëñ  ùëîùëéùëñùëõ  \\\n",
      "0           0           0           0           0  ...       0    0     0   \n",
      "1           0           0           0           0  ...       0    0     0   \n",
      "2           0           0           0           0  ...       0    0     0   \n",
      "3           0           0           0           0  ...       0    0     0   \n",
      "4           0           0           0           0  ...       0    0     0   \n",
      "\n",
      "   ùëöùëíùëéùë†ùë¢ùëüùëí  ùëùùëúùë†ùë°ùë°ùëíùë†ùë°  ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ  ùëùùëüùëíùë°ùëíùë†ùë°  ùëüùëíùëêùëéùëôùëô  ùë†ùëêùëúùëüùëí  ùüéùüíùüï  \n",
      "0        0         0          0        0       0      0    0  \n",
      "1        0         0          0        0       0      0    0  \n",
      "2        0         0          0        0       0      0    0  \n",
      "3        0         0          0        0       0      0    0  \n",
      "4        0         0          0        0       0      0    0  \n",
      "\n",
      "[5 rows x 5677 columns]\n",
      "   Unnamed: 0  Unnamed: 1  Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5  \\\n",
      "0    0.000000         0.0         0.0         0.0         0.0         0.0   \n",
      "1    1.000000         0.0         0.0         0.0         0.0         0.0   \n",
      "2    1.693147         0.0         0.0         0.0         0.0         0.0   \n",
      "3    2.098612         0.0         0.0         0.0         0.0         0.0   \n",
      "4    2.386294         0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "   Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  ...  ùëÖùëíùëêùëéùëôùëô  ùëáùêπùëñ  ùëîùëéùëñùëõ  \\\n",
      "0         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "1         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "2         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "3         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "4         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "\n",
      "   ùëöùëíùëéùë†ùë¢ùëüùëí  ùëùùëúùë†ùë°ùë°ùëíùë†ùë°  ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ  ùëùùëüùëíùë°ùëíùë†ùë°  ùëüùëíùëêùëéùëôùëô  ùë†ùëêùëúùëüùëí  ùüéùüíùüï  \n",
      "0      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "1      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "2      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "3      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "4      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "\n",
      "[5 rows x 5677 columns]\n"
     ]
    }
   ],
   "source": [
    "# 6) use the \"applymap\" function of the dataframe to apply the function \n",
    "# above to each cell of the table\n",
    "df_csv = pd.read_csv('word-by-chunk.csv')\n",
    "print(df_csv.head())\n",
    "df_weight_freq = df_csv.applymap(weight_word_freq)\n",
    "print(df_weight_freq.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1.6931471805599454\n"
     ]
    }
   ],
   "source": [
    "# 7) check that the numbers in the resulting matrix look accurate;\n",
    "# print the value before and after applying the function above\n",
    "\n",
    "print(df_csv.loc[0, 'time'])\n",
    "print(df_weight_freq.loc[0, 'time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Matrix normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) look at the image below; why do you think that we need to normalize our \n",
    "# data before clustering in this particular case? \n",
    "# So that it is easier to compare and there is less variabiles to consider between vectors.\n",
    "# You don't have to worry about length, just the angles between vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.stack.imgur.com/N2unM.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, it's common practice to normalize your data before clustering - so that variables are comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) describe how the min-max normalization works:\n",
    "# this maps the max x-value to 1 and the min x-value to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/media/aml-normalization-minmax.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) describe how normalizing using a z-score works:\n",
    "# Values are converted such that the average is 0 and the standard deviation becomes 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*13XKCXQc7eabfZbRzkvGvA.gif\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) describe how normalizing to unit norm works\n",
    "# this method is like the one we learned in Udacity. You find the magnitude and divide all components by the \n",
    "# magnitude, giving you a vector of magnitude 1 in the same direction of the original vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources: \n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer\n",
    "* http://mathworld.wolfram.com/NormalVector.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with some pre-made normalization functions from sklearn (feel free to skim this page):\n",
    "* https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Users/erincarvalho/.pyenv/versions/3.6.4/lib/python3.6/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/erincarvalho/.pyenv/versions/3.6.4/lib/python3.6/site-packages (from sklearn) (0.20.3)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /Users/erincarvalho/.pyenv/versions/3.6.4/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in /Users/erincarvalho/.pyenv/versions/3.6.4/lib/python3.6/site-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "   Unnamed: 0  Unnamed: 1  Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5  \\\n",
      "0    0.000000         0.0         0.0         0.0         0.0         0.0   \n",
      "1    1.000000         0.0         0.0         0.0         0.0         0.0   \n",
      "2    1.693147         0.0         0.0         0.0         0.0         0.0   \n",
      "3    2.098612         0.0         0.0         0.0         0.0         0.0   \n",
      "4    2.386294         0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "   Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  ...  ùëÖùëíùëêùëéùëôùëô  ùëáùêπùëñ  ùëîùëéùëñùëõ  \\\n",
      "0         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "1         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "2         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "3         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "4         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "\n",
      "   ùëöùëíùëéùë†ùë¢ùëüùëí  ùëùùëúùë†ùë°ùë°ùëíùë†ùë°  ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ  ùëùùëüùëíùë°ùëíùë†ùë°  ùëüùëíùëêùëéùëôùëô  ùë†ùëêùëúùëüùëí  ùüéùüíùüï  \n",
      "0      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "1      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "2      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "3      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "4      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "\n",
      "[5 rows x 5677 columns]\n",
      "   Unnamed: 0  Unnamed: 1  Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5  \\\n",
      "0    0.000000         0.0         0.0         0.0         0.0         0.0   \n",
      "1    0.358197         0.0         0.0         0.0         0.0         0.0   \n",
      "2    0.806794         0.0         0.0         0.0         0.0         0.0   \n",
      "3    0.879444         0.0         0.0         0.0         0.0         0.0   \n",
      "4    0.914486         0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "   Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  ...  ùëÖùëíùëêùëéùëôùëô  ùëáùêπùëñ  ùëîùëéùëñùëõ  \\\n",
      "0         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "1         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "2         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "3         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "4         0.0         0.0         0.0         0.0  ...     0.0  0.0   0.0   \n",
      "\n",
      "   ùëöùëíùëéùë†ùë¢ùëüùëí  ùëùùëúùë†ùë°ùë°ùëíùë†ùë°  ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ  ùëùùëüùëíùë°ùëíùë†ùë°  ùëüùëíùëêùëéùëôùëô  ùë†ùëêùëúùëüùëí  ùüéùüíùüï  \n",
      "0      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "1      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "2      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "3      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "4      0.0       0.0        0.0      0.0     0.0    0.0  0.0  \n",
      "\n",
      "[5 rows x 5677 columns]\n"
     ]
    }
   ],
   "source": [
    "# 12) since we are working with vectors, apply the Normalizer from \n",
    "# sklearn.preprocessing to our dataframe. Print a few values \n",
    "# before and after to make sure you've applied the normalization\n",
    "!pip install sklearn\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "print(df_weight_freq.head())\n",
    "normalizer = Normalizer(norm='max')\n",
    "\n",
    "df_norm = pd.DataFrame(normalizer.fit_transform(df_weight_freq),columns=df_weight_freq.columns)\n",
    "print(df_norm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 13) create a function that takes a dataframe as argument and where a second\n",
    "# argument is the type of normalization (MinMaxScaler, Normalizer, StandardScaler)\n",
    "# and returns the normalized dataframe\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, StandardScaler\n",
    "\n",
    "def normalization(dataframe, norm_method):\n",
    "    normalizer = norm_method()\n",
    "    df_normalized = pd.DataFrame(normalizer.fit_transform(dataframe),columns=dataframe.columns)\n",
    "    return df_normalized\n",
    "\n",
    "# print(normalization(df_weight_freq, MinMaxScaler))\n",
    "# print(normalization(df_weight_freq, Normalizer))\n",
    "# print(normalization(df_weight_freq, StandardScaler))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Deviation Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.dropbox.com/s/9f73r7pk7bi7vh9/deviation_vectors.png?dl=1\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0     2216.792377\n",
      "Unnamed: 1        1.185767\n",
      "Unnamed: 2        1.112245\n",
      "Unnamed: 3        1.112245\n",
      "Unnamed: 4        1.112245\n",
      "Unnamed: 5        1.112245\n",
      "Unnamed: 6        1.112245\n",
      "Unnamed: 7        1.112245\n",
      "Unnamed: 8        0.494919\n",
      "Unnamed: 9        0.875983\n",
      "Unnamed: 10       0.494919\n",
      "Unnamed: 11       0.875983\n",
      "Unnamed: 12       0.494919\n",
      "Unnamed: 13       0.875983\n",
      "Unnamed: 14       0.494971\n",
      "Unnamed: 15       0.494971\n",
      "Unnamed: 16       1.185767\n",
      "Unnamed: 17       0.494971\n",
      "Unnamed: 18       1.185767\n",
      "Unnamed: 19       0.494971\n",
      "Unnamed: 20       1.112245\n",
      "Unnamed: 21       1.185767\n",
      "Unnamed: 22       0.494971\n",
      "Unnamed: 23       1.112130\n",
      "abilities         2.710555\n",
      "ability          19.693197\n",
      "able              9.706820\n",
      "abnormal          0.521503\n",
      "absence           1.194818\n",
      "absent            1.074738\n",
      "                  ...     \n",
      "ztransform        1.028590\n",
      "zurich            0.501395\n",
      "¬µŒ∏t               0.843740\n",
      "√©cole             0.588973\n",
      "Œ±jt               0.593349\n",
      "Œ±xt               1.005107\n",
      "Œ∏lj               1.172284\n",
      "Œªkak              0.526751\n",
      "Œªkz               1.012819\n",
      "œÉŒ∏t               0.843740\n",
      "‚Äòamount           0.610956\n",
      "‚Äòhomework         0.584865\n",
      "‚Äòlarge            0.611687\n",
      "‚Äòlives            0.541686\n",
      "‚ààgt               0.892188\n",
      "‚àëùêøùëó               0.869071\n",
      "‚àëùëòùëó               0.512700\n",
      "‚äÇsi               0.592507\n",
      "ùêºùê∑ùêπ               0.514862\n",
      "ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ         0.512046\n",
      "ùëÖùëíùëêùëéùëôùëô            0.512046\n",
      "ùëáùêπùëñ               0.514786\n",
      "ùëîùëéùëñùëõ              0.475553\n",
      "ùëöùëíùëéùë†ùë¢ùëüùëí           0.512046\n",
      "ùëùùëúùë†ùë°ùë°ùëíùë†ùë°          0.475553\n",
      "ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ         0.866970\n",
      "ùëùùëüùëíùë°ùëíùë†ùë°           0.805182\n",
      "ùëüùëíùëêùëéùëôùëô            0.866970\n",
      "ùë†ùëêùëúùëüùëí             0.998002\n",
      "ùüéùüíùüï               0.473744\n",
      "Length: 5677, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 14) compute the sum of the vectors\n",
    "\n",
    "sum_vector = df_norm.sum()\n",
    "print(sum_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0     0.390487\n",
      "Unnamed: 1     0.000209\n",
      "Unnamed: 2     0.000196\n",
      "Unnamed: 3     0.000196\n",
      "Unnamed: 4     0.000196\n",
      "Unnamed: 5     0.000196\n",
      "Unnamed: 6     0.000196\n",
      "Unnamed: 7     0.000196\n",
      "Unnamed: 8     0.000087\n",
      "Unnamed: 9     0.000154\n",
      "Unnamed: 10    0.000087\n",
      "Unnamed: 11    0.000154\n",
      "Unnamed: 12    0.000087\n",
      "Unnamed: 13    0.000154\n",
      "Unnamed: 14    0.000087\n",
      "Unnamed: 15    0.000087\n",
      "Unnamed: 16    0.000209\n",
      "Unnamed: 17    0.000087\n",
      "Unnamed: 18    0.000209\n",
      "Unnamed: 19    0.000087\n",
      "Unnamed: 20    0.000196\n",
      "Unnamed: 21    0.000209\n",
      "Unnamed: 22    0.000087\n",
      "Unnamed: 23    0.000196\n",
      "abilities      0.000477\n",
      "ability        0.003469\n",
      "able           0.001710\n",
      "abnormal       0.000092\n",
      "absence        0.000210\n",
      "absent         0.000189\n",
      "                 ...   \n",
      "ztransform     0.000181\n",
      "zurich         0.000088\n",
      "¬µŒ∏t            0.000149\n",
      "√©cole          0.000104\n",
      "Œ±jt            0.000105\n",
      "Œ±xt            0.000177\n",
      "Œ∏lj            0.000206\n",
      "Œªkak           0.000093\n",
      "Œªkz            0.000178\n",
      "œÉŒ∏t            0.000149\n",
      "‚Äòamount        0.000108\n",
      "‚Äòhomework      0.000103\n",
      "‚Äòlarge         0.000108\n",
      "‚Äòlives         0.000095\n",
      "‚ààgt            0.000157\n",
      "‚àëùêøùëó            0.000153\n",
      "‚àëùëòùëó            0.000090\n",
      "‚äÇsi            0.000104\n",
      "ùêºùê∑ùêπ            0.000091\n",
      "ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ      0.000090\n",
      "ùëÖùëíùëêùëéùëôùëô         0.000090\n",
      "ùëáùêπùëñ            0.000091\n",
      "ùëîùëéùëñùëõ           0.000084\n",
      "ùëöùëíùëéùë†ùë¢ùëüùëí        0.000090\n",
      "ùëùùëúùë†ùë°ùë°ùëíùë†ùë°       0.000084\n",
      "ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ      0.000153\n",
      "ùëùùëüùëíùë°ùëíùë†ùë°        0.000142\n",
      "ùëüùëíùëêùëéùëôùëô         0.000153\n",
      "ùë†ùëêùëúùëüùëí          0.000176\n",
      "ùüéùüíùüï            0.000083\n",
      "Length: 5677, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 15) normalize the vector (find its average)\n",
    "\n",
    "avg_vector = sum_vector/len(df_norm.columns)\n",
    "print(avg_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  Unnamed: 1  Unnamed: 2  Unnamed: 3  Unnamed: 4  Unnamed: 5  \\\n",
      "0      -0.390487   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "1      -0.032290   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2       0.416307   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "3       0.488957   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "4       0.523999   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "5       0.495297   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "6       0.557186   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "7       0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "8       0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "9       0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "10      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "11      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "12      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "13      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "14      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "15      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "16      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "17      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "18      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "19      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "20      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "21      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "22      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "23      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "24      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "25      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "26      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "27      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "28      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "29      0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "...          ...         ...         ...         ...         ...         ...   \n",
      "2189    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2190    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2191    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2192    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2193    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2194    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2195    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2196    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2197    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2198    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2199    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2200    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2201    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2202    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2203    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2204    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2205    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2206    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2207    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2208    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2209    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2210    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2211    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2212    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2213    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2214    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2215    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2216    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2217    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "2218    0.609513   -0.000209   -0.000196   -0.000196   -0.000196   -0.000196   \n",
      "\n",
      "      Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  ...   ùëÖùëíùëêùëéùëôùëô       ùëáùêπùëñ  \\\n",
      "0      -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "1      -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2      -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "3      -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "4      -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "5      -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "6      -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "7      -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "8      -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "9      -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "10     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "11     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "12     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "13     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "14     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "15     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "16     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "17     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "18     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "19     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "20     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "21     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "22     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "23     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "24     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "25     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "26     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "27     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "28     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "29     -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "...          ...         ...         ...         ...  ...      ...       ...   \n",
      "2189   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2190   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2191   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2192   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2193   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2194   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2195   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2196   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2197   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2198   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2199   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2200   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2201   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2202   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2203   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2204   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2205   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2206   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2207   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2208   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2209   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2210   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2211   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2212   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2213   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2214   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2215   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2216   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2217   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "2218   -0.000196   -0.000196   -0.000087   -0.000154  ... -0.00009 -0.000091   \n",
      "\n",
      "          ùëîùëéùëñùëõ  ùëöùëíùëéùë†ùë¢ùëüùëí  ùëùùëúùë†ùë°ùë°ùëíùë†ùë°  ùëùùëüùëíùëêùëñùë†ùëñùëúùëõ   ùëùùëüùëíùë°ùëíùë†ùë°    ùëüùëíùëêùëéùëôùëô     ùë†ùëêùëúùëüùëí  \\\n",
      "0    -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "1    -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2    -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "3    -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "4    -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "5    -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "6    -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "7    -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "8    -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "9    -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "10   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "11   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "12   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "13   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "14   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "15   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "16   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "17   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "18   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "19   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "20   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "21   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "22   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "23   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "24   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "25   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "26   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "27   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "28   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "29   -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "...        ...      ...       ...        ...       ...       ...       ...   \n",
      "2189 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2190 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2191 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2192 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2193 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2194 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2195 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2196 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2197 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2198 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2199 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2200 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2201 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2202 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2203 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2204 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2205 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2206 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2207 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2208 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2209 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2210 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2211 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2212 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2213 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2214 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2215 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2216 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2217 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "2218 -0.000084 -0.00009 -0.000084  -0.000153 -0.000142 -0.000153 -0.000176   \n",
      "\n",
      "           ùüéùüíùüï  \n",
      "0    -0.000083  \n",
      "1    -0.000083  \n",
      "2    -0.000083  \n",
      "3    -0.000083  \n",
      "4    -0.000083  \n",
      "5    -0.000083  \n",
      "6    -0.000083  \n",
      "7    -0.000083  \n",
      "8    -0.000083  \n",
      "9    -0.000083  \n",
      "10   -0.000083  \n",
      "11   -0.000083  \n",
      "12   -0.000083  \n",
      "13   -0.000083  \n",
      "14   -0.000083  \n",
      "15   -0.000083  \n",
      "16   -0.000083  \n",
      "17   -0.000083  \n",
      "18   -0.000083  \n",
      "19   -0.000083  \n",
      "20   -0.000083  \n",
      "21   -0.000083  \n",
      "22   -0.000083  \n",
      "23   -0.000083  \n",
      "24   -0.000083  \n",
      "25   -0.000083  \n",
      "26   -0.000083  \n",
      "27   -0.000083  \n",
      "28   -0.000083  \n",
      "29   -0.000083  \n",
      "...        ...  \n",
      "2189 -0.000083  \n",
      "2190 -0.000083  \n",
      "2191 -0.000083  \n",
      "2192 -0.000083  \n",
      "2193 -0.000083  \n",
      "2194 -0.000083  \n",
      "2195 -0.000083  \n",
      "2196 -0.000083  \n",
      "2197 -0.000083  \n",
      "2198 -0.000083  \n",
      "2199 -0.000083  \n",
      "2200 -0.000083  \n",
      "2201 -0.000083  \n",
      "2202 -0.000083  \n",
      "2203 -0.000083  \n",
      "2204 -0.000083  \n",
      "2205 -0.000083  \n",
      "2206 -0.000083  \n",
      "2207 -0.000083  \n",
      "2208 -0.000083  \n",
      "2209 -0.000083  \n",
      "2210 -0.000083  \n",
      "2211 -0.000083  \n",
      "2212 -0.000083  \n",
      "2213 -0.000083  \n",
      "2214 -0.000083  \n",
      "2215 -0.000083  \n",
      "2216 -0.000083  \n",
      "2217 -0.000083  \n",
      "2218 -0.000083  \n",
      "\n",
      "[2219 rows x 5677 columns]\n"
     ]
    }
   ],
   "source": [
    "# 16) take each vector and subtract its components along v_avg\n",
    "dev_vector = df_norm - avg_vector\n",
    "print(dev_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) put the code above in a function that takes in a dataframe as an argument\n",
    "# and computes deviation vectors of each row (=document)\n",
    "\n",
    "def deviation_vectors(dataframe):\n",
    "    normalizer = Normalizer(norm='max')\n",
    "    df_normalized = pd.DataFrame(normalizer.fit_transform(dataframe),columns=dataframe.columns)\n",
    "    sum_vector = df_normalized.sum()\n",
    "    avg_vector = sum_vector/len(df_normalized.columns)\n",
    "    dev_vector = df_normalized - avg_vector\n",
    "    return dev_vector\n",
    "\n",
    "# print(deviation_vectors(df_weight_freq))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEEK 7 - CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step - Putting it all together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
